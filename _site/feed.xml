<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-03-06T14:37:31+01:00</updated><id>http://localhost:4000/feed.xml</id><entry><title type="html">Beginners Guide to Feature Selection and Categorical Embeddings with Project on Unclean Structured Data</title><link href="http://localhost:4000/feature-selection-on-categorical-structured-data/" rel="alternate" type="text/html" title="Beginners Guide to Feature Selection and Categorical Embeddings with Project on Unclean Structured Data" /><published>2021-03-04T00:00:00+01:00</published><updated>2021-03-04T00:00:00+01:00</updated><id>http://localhost:4000/feature%20selection%20on%20categorical%20structured%20data</id><content type="html" xml:base="http://localhost:4000/feature-selection-on-categorical-structured-data/">&lt;p&gt;Have you ever been faced with such a dataset so unclean and irregular, that you feel terrified? ðŸ˜¯. If your answer is NO, you have either been practicing deep learning for over 3 years or you rarely ever practice at all? But jokes aside, we deep learners frequently get faced with rather unclean data that needs tedious amounts of processing to be usable. Several of these problems which we are gonna tackle in this tutorial by solving a machine learning problem in a beginner-friendly way. letâ€™s get started!&lt;/p&gt;

&lt;p&gt;In this Tutorial(We Cover the Foll):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Real World Structured Data&lt;/li&gt;
  &lt;li&gt;Example Project&lt;/li&gt;
  &lt;li&gt;Dataset Visualization with Sns and Pandas&lt;/li&gt;
  &lt;li&gt;Tackling Nan Values&lt;/li&gt;
  &lt;li&gt;Performing Categorical Embeddings&lt;/li&gt;
  &lt;li&gt;Data Imputation Methods&lt;/li&gt;
  &lt;li&gt;How to select priority features (Feature Selection)&lt;/li&gt;
  &lt;li&gt;Fitting Model&lt;/li&gt;
  &lt;li&gt;Feedback and Summary&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;NOTE: If at any point in time, you feel confused, make sure to check my Colab notebook and follow along with it so you can see the full transfer of data between data frames&lt;/strong&gt;
&lt;a href=&quot;https://colab.research.google.com/drive/1ZqMuTzZpzaTnubjXlfZ49MuAisfXGPpx#scrollTo=4PMMzA96mT6A&quot;&gt;Full Notebook Summary&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;structured-data-in-the-real-world&quot;&gt;Structured Data in the Real World&lt;/h3&gt;
&lt;p&gt;Structured Data is Data that can be tabulated or visualized in a table format. In machine learning, this term refers to numerical Data also known as Continous Values. Structured Data is logically the opposite of unstructured data, which refers to data that can not be technically tabularized or which has an irregular format, examples are, Images, text and videos data&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Structured data conforms to a tabular format with a relationship between the different rows and columns. Common examples of structured data are Excel files or SQL databases.
â€” &lt;cite&gt;Big Data Framework&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the real world, we gather a lot of structured data, which follows the laws of the real world. When they are collected, no consideration is given to how they can be used for machine learning and collecting insights. An Example: Imagine, a law firm kept the records on all their customers in the year 2019, with information on say, how long the companyâ€™s representatives talked on the phone with customers, how much money customers invested for insurance, etc. In a record as this, there are natural data that are not readily convertible for working with. In this case, employing machine learning cleaning methods is the only viable open. Letâ€™s dive into the subject data for this tutorial.&lt;/p&gt;

&lt;h3 id=&quot;example-project&quot;&gt;Example Project&lt;/h3&gt;
&lt;p&gt;The dataset for this tutorial contains two files:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;train.csv: 6500 X 20&lt;/li&gt;
  &lt;li&gt;test.csv: 3500 X 19&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In other words, we have a training data frame with 6500 rows and 20 columns and a testing/evaluating data frame with 3500 rows and 19 columns.&lt;/p&gt;

&lt;h6 id=&quot;the-task&quot;&gt;The task&lt;/h6&gt;
&lt;p&gt;You work for a company that sells sculptures that are acquired from various artists around the world. Your task is to predict the cost required to ship these sculptures to customers based on the information provided in the dataset.
The data frames contain several columns which depict the features we are working with, and we need to build a model that predicts the last column in the test set(Cost of Sculptures). Letâ€™s Visualize the dataset to see what it looks like.&lt;/p&gt;

&lt;h4 id=&quot;dataset-visualization-with-sns-and-pandas&quot;&gt;Dataset Visualization with Sns and Pandas&lt;/h4&gt;

&lt;p&gt;To visualize the dataset, we should load it up, import the necessary packages &amp;amp; modules. Iâ€™m working on Colab for this tutorial and the dataset is stored in my drive. To follow along with this guide , download dataset &lt;a href=&quot;https://drive.google.com/file/d/12q8kY1MYKpyTMPvk8aNDa-TindyAmsGg/view?usp=sharing&quot;&gt;here&lt;/a&gt;. Upload the zip file to drive&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The first thing we need to do is to mount Drive:&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;google.colab&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;drive&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;drive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/content/drive'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Import necessary packages which we would use throughout the project&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt; 
&lt;span class=&quot;c1&quot;&gt;# Use seaborn for pairplot
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;seaborn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Next, letâ€™s import the Zip module and extract the dataset. On this occasion, after having learned how using the alias name â€˜zipâ€™ when calling ZipFile class result in errors for later use, I decided to use a different name i.e grab.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;zipfile&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ZipFile&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;file_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'/path/to/dataset'&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ZipFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;grab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;extractall&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/path/'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Done'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Now with the dataset loaded, we can run some visualizations using pandas and sns. First, letâ€™s take a look at the columns in the train.csv file using pandas to read , draw the head, then quickly check for Nan Values.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;raw_dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/path/to/extracted/csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;raw_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Checking for nan values
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datasetisnull&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In the image below, we can see the total of 20 columns and their features; &lt;strong&gt;Customer ID, Artist Name, Artist Reputation, Width, Height, Width, Material, Price of Sculpture, Base Shipping Price, International, Express Shipment, Installation Included, Transport, Fragile, Customer Information, Remote Location, Scheduled Date, Delivery Date, Customer Location&lt;/strong&gt;. We also have a lot of Nan Values amongst our data, this would be tackled after dropping irrelevant columns with string values and over-tedious formats like a date.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;tackling-nan-values--irrelevant-features&quot;&gt;Tackling Nan Values &amp;amp; Irrelevant features&lt;/h3&gt;

&lt;p&gt;Some particular columns contain string values and other formats that are irrelevant in predicting the cost of the artwork.&lt;/p&gt;

&lt;p&gt;For this next step, we have two options to consider, should we:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Start Categorical Embeddings&lt;/li&gt;
  &lt;li&gt;Or do we first drop columns we wouldnâ€™t need.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Letâ€™s approach this somewhat logically: If we start with the first option, we are likely going to meet with columns that canâ€™t be(or rather, too tedious) to embed. But if we drop them first, we can exclude those absurdly irrelevant features before moving on embedding the relevant portion of columns.&lt;/p&gt;

&lt;p&gt;Before Dropping Columns in the training and test set, letâ€™s first pop-out our target column (Cost) and save it. Later we can reference it for fitting our model.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;raw_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Cost'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;raw_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In the above code block, we separated our cost from the main train data frame and stored it in a new df. On drawing the head though, we can see something strange at work!. The cost consists of both negative and positive values, this wonâ€™t do, we want &lt;strong&gt;Only&lt;/strong&gt; positive values. As I said earlier real-world data never comes the way you expect. This could just be a wrong entry by a tired cashier or a more acceptable answer perhaps would be that (For all Art Sculptures that were delivered with defects, the company holds the loss?). Anyway, we can easily correct this using the absolute function in pandas. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;The absolute of any number is = the **positive of that number**&lt;/code&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;train_Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Cost'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;With the above code, our target variable now contains only positive values. Now thatâ€™s taken care of, we can proceed to drop special irrelevant columns from our training set&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;  
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;raw_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Cost'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Customer Id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Artist Name'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Delivery Date'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Scheduled Date'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Customer Location'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Earlier, I checked our columns for nana values. In total it returned about 4000 Nan Values. But in every column we see that nan values are occurring more in some especially (Material, Transport, Remote_Location, Width, Height, etc). For this step, we want to replace nan values in &lt;em&gt;First&lt;/em&gt; Columns with more than two classes. Material, Remote_Location, and Transport fit the description, the reason is this, if we embed them we stand a chance of losing the relative relationship in our features. To do this, we are gonna create a simple function that fills Nan values with the highest occurring category in the column. e.g In the Material column, we have 7 classes (Brass, Stone, Aluminium, Bronze, Clay, ). Our function loops through the rows and replaces Nan Values with the highest repeating class.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c1&quot;&gt;#Function to replace NAN values with mode value
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;replace_nan_most_freq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ColName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;most_frequent_category&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ColName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nan&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;most&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;occured&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;category&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ColName&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;-Imputed&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ColName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ColName&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;-Imputed&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fillna&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;most_frequent_category&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#Call function to impute most occured category
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Columns&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Material'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Remote Location'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Transport'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;replace_nan_most_freq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;replace_nan_most_freq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Display imputed result
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Material'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Material-Imputed'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Remote Location'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Remote Location-Imputed'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Transport'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Transport-Imputed'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Material'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Material-Imputed'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Remote Location'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Remote Location-Imputed'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Transport'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Transport-Imputed]].head(10)
#Drop actual columns
train = train.drop(['&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Material&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;', '&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Remote&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Location&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;','&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Transport&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'], axis = 1)
test = test.drop(['&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Material&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;', '&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Remote&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Location&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;','&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Transport&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'], axis = 1)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;With that, if we plot the train and test df, we get the following:
As you can see Material has been replaced by Material_Imputed, likewise with Transport and Remote Location.;
&lt;img src=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;categorical-embeddings&quot;&gt;Categorical Embeddings&lt;/h3&gt;

&lt;p&gt;In categorical Embedding, we want to check for columns with a data type of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;object&lt;/code&gt; and label encode them using scikitâ€™s learn Label Encoder.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c1&quot;&gt;# Get list of categorical variables
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtypes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'object'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;object_cols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Categorical variables:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;object_cols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LabelEncoder&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Make copy to avoid changing original data 
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label_X_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;label_X_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Apply label encoder to each column with categorical data
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label_encoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LabelEncoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;object_cols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;label_X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label_encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;label_X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label_encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;With that, plotting the head gives us this: 
&lt;img src=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;All category columns have been lav\bel encoded to numerical values. The last step is to fill up the remaining columns that still contain Nan Values like Width, Height, Weight. Because, if you can remember, we only replaced the category nan values above. The code below uses sklearnâ€™s Simple Impueter method.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.impute&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SimpleImputer&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Imputation
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;my_imputer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SimpleImputer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;imputed_X_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;my_imputer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label_X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;imputed_X_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;my_imputer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label_X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Imputation removed column names; put them back
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imputed_X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;imputed_X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Now if we check for Nan values, it returns zero across all columns.&lt;/p&gt;

&lt;h3 id=&quot;feature-selection&quot;&gt;Feature Selection&lt;/h3&gt;

&lt;p&gt;With that taken care of, we can now start doing some feature selection and deciding which columns are of no use to us. A lot of logical thinking is important as this is a real-world problem and real-world insight is required.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;NOTE: Any column we decide to drop must be reflected in the test set. Likewise, any preprocessing step we take. As this data(test set) is what we would be making predictions on, and inconsistent columns would result in errors.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-select-priority-features-methods-you-can-use&quot;&gt;How to select Priority Features (Methods you can use)&lt;/h2&gt;
&lt;p&gt;When faced with a feature selection problem in deep learning and machine learning, there are several methods you can apply to arrive at better features during training, therefore, better model accuracy.&lt;/p&gt;

&lt;p&gt;We have &lt;em&gt;now&lt;/em&gt; 19 columns in our training set after dropping Cost.&lt;/p&gt;

&lt;h4 id=&quot;description-of-variables-in-the-above-file&quot;&gt;Description of variables in the above file&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Customer ID&lt;/strong&gt;: A set of unique values associated with every customer (This adds absolutely nothing of value)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Artist Name&lt;/strong&gt;: The name of the Artist who created the artwork&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Artist Reputation&lt;/strong&gt;: A float value important for understanding how expensive an art would be&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Weight&lt;/strong&gt;: How heavy an Artwork is, but as humans, we know that heavier artwork doesnâ€™t necessarily mean more expensive. But still a great judge of worth.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Width&lt;/strong&gt;: Width of Artwork&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Height&lt;/strong&gt;: height of the artwork&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Material&lt;/strong&gt;: A very important feature&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Base Shipping Price&lt;/strong&gt;: The original cost for shipping artwork from one location to another. It varies (Important)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;International&lt;/strong&gt;: Geographic location of a customer. Across borders means extra customs cost (Important)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Express Shipment&lt;/strong&gt;: Value depicting if a Customer requested for express shipment (Important)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Installation Included&lt;/strong&gt;: Should artwork be installed along with delivery? (Important)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fragile&lt;/strong&gt;: How fragile the artwork is. Just like weight, more fragile doesnâ€™t mean more expensive. Itâ€™s a shaky yet important feature (Stable)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Transport&lt;/strong&gt;: What means of transportation is used to deliver Artwork. Airplanes generally cost more (Important)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Remote Location&lt;/strong&gt;: What kind of Environment Customer resides. The lesser accessible the more cost it takes?. This feature is a little bit unspecific because it could easily be the other way round. A poorer customer is less likely to pay more. (Stable)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Customer Information&lt;/strong&gt;: How financially stable is the purchaser. More means more likely to give tips, request installations, quicker delivery, etc. (Important)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Customer Location&lt;/strong&gt;: Where a customer resides. A bunch of specific locations that would be too tedious to embed. Also, Remote Location gathers similar Information (Redundant)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Delivery Date&lt;/strong&gt;: Date purchaser ordered for ar work to be delivered. A good &lt;strong&gt;BUT&lt;/strong&gt; redundant feature. features like express shipment and installations are better judges of how quickly customer wants artwork (Redundant)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scheduled Date&lt;/strong&gt;: Same as Delivery Date (Redundant)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The above can be achieved with little logical deductions and insights, but as you can see, there are still some uncertainties. By using methods, tests, and libraries we have a better ground to decide.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;h3&gt;Univariate Selection&lt;/h3&gt;
    &lt;p&gt;Statistical tests can be used to select those features that have the strongest relationship with the output variable.
The scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features.
The example below uses the f_classif statistical test for positive numerical features to select 10 of the best features from the Art Exhibition Dataset. Some of the feature columns must be dropped before you can use this method: Those that are string values e.g Customer Id and Artist Name.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c1&quot;&gt;#apply SelectKBest class to extract top 10 best features
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.feature_selection&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SelectKBest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_classif&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bestfeatures&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SelectKBest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score_func&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f_classif&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bestfeatures&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_final&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dfscores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dfcolumns&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_final&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#concat two dataframes for better visualization 
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;featureScores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dfcolumns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dfscores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;featureScores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Specs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Score'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;#naming the dataframe columns
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;featureScores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nlargest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Score'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;#print 10 best features&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The top ten best features are displayed below helping us get rid of uncertainties like the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Fragile&lt;/code&gt; column.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;h3&gt;Feature Importance&lt;/h3&gt;
    &lt;p&gt;You can get the feature importance of each feature of your dataset by using the feature importance property of the model.
Feature importance gives you a score for each feature of your data, the higher the score, the more important or relevant is the feature towards your output variable.
Feature importance is an inbuilt class that comes with Tree-Based Classifiers, we will be using Extra Tree Regressor for extracting the top 10 features for the dataset.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.ensemble&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ExtraTreesRegressor&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ExtraTreesRegressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_final&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_importances_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#use inbuilt class feature_importances of tree based classifiers
#plot graph of feature importances for better visualization
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_importances&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Series&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_importances_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_final&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;feat_importances&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nlargest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'barh'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Again, we see that some certain feature columns are among the top 10 using this selection method. We now know those very important features we should target when we limit the training features to say, 10.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;h3&gt;Correlation Matrix with Heatmap&lt;/h3&gt;
    &lt;p&gt;Correlation states how the features are related to each other or the target variable.
Correlation can be positive (increase in one value of feature increases the value of the target variable) or negative (increase in one value of feature decreases the value of the target variable)
Heatmap makes it easy to identify which features are most related to the target variable, we will plot a heatmap of correlated features using the seaborn library.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c1&quot;&gt;#get correlations of each features in dataset
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corrmat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;raw_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;top_corr_features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;corrmat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#plot heat map
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heatmap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;top_corr_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;annot&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;RdYlGn&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now, we are talking. Letâ€™s make a round-up of the 11 features we want to use for training, by selecting those occurring the most after the feature selection methods above.&lt;/p&gt;

&lt;h3 id=&quot;ten-best-features-to-use&quot;&gt;Ten Best Features to Use&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Price of Sculpture&lt;/li&gt;
  &lt;li&gt;weight&lt;/li&gt;
  &lt;li&gt;Artist reputation&lt;/li&gt;
  &lt;li&gt;Base Shipping Price&lt;/li&gt;
  &lt;li&gt;Width&lt;/li&gt;
  &lt;li&gt;Height&lt;/li&gt;
  &lt;li&gt;Express Shipment&lt;/li&gt;
  &lt;li&gt;International&lt;/li&gt;
  &lt;li&gt;Transport_Imputed&lt;/li&gt;
  &lt;li&gt;material_Imputed&lt;/li&gt;
  &lt;li&gt;Customer Information.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;fitting-the-model&quot;&gt;Fitting the Model&lt;/h3&gt;

&lt;p&gt;We would quickly drop the remaining columns while leaving the above, and Finally, proceed with scaling our data &amp;amp; fitting our model with sklearn StandardScaler and Random Forest Regressor respectively.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;imputed_X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Fragile'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Remote Location'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Installation Included'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;imputed_X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Fragile'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Remote Location'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Installation Included'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preprocessing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StandardScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imputed_X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imputed_X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imputed_X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imputed_X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Location&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;','&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Installation&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Included&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'], axis=1)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c1&quot;&gt;#Import Random Forest Model
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.ensemble&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestRegressor&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#Create a Gaussian Classifier
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rgf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RandomForestRegressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#Train the model using the training sets y_pred=clf.predict(X_test)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rgf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Last But not least, we need to evaluate our model on the test set, prepare it and arrange it in a new data frame with two columns Customer Id and Cost.&lt;/p&gt;

&lt;p&gt;And now this is how the final model looks like. You can download the .csv file &lt;a href=&quot;&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;feedback-and-summary&quot;&gt;Feedback and Summary&lt;/h3&gt;
&lt;p&gt;We have had fun with this project, cleaning, visualizing, dropping, performing feature engineering, and learning how to use the sklearn library for Machine Learning work. Itâ€™s a comprehensive guide where I decided to tackle problems to show you how you can solve similar problems. Naturally, you have questions to ask, and Iâ€™m only happy to see you ask them. The comments box below is available for your chats. Thanks for reading this article, I hoped it has achieved its purpose of guiding you through the concept of feature selection engineering in deep learning, Chel.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The above quote is a definition extracted from Big Data Frameworkâ€™s &lt;a href=&quot;https://www.bigdataframework.org/data-types-structured-vs-unstructured-data/&quot;&gt;article&lt;/a&gt; written on January 9th, 2019.Â &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Chel</name></author><summary type="html">Have you ever been faced with such a dataset so unclean and irregular, that you feel terrified? ðŸ˜¯. If your answer is NO, you have either been practicing deep learning for over 3 years or you rarely ever practice at all? But jokes aside, we deep learners frequently get faced with rather unclean data that needs tedious amounts of processing to be usable. Several of these problems which we are gonna tackle in this tutorial by solving a machine learning problem in a beginner-friendly way. letâ€™s get started!</summary></entry><entry><title type="html">Convolutional Neural Networks Vs Recurrent Neural Networks(An Intuitive Guide for every Deep Learning Practitioner)</title><link href="http://localhost:4000/cnn-and-rnn-simplified/" rel="alternate" type="text/html" title="Convolutional Neural Networks Vs Recurrent Neural Networks(An Intuitive Guide for every Deep Learning Practitioner)" /><published>2020-10-14T00:00:00+01:00</published><updated>2020-10-14T00:00:00+01:00</updated><id>http://localhost:4000/cnn%20and%20rnn%20simplified</id><content type="html" xml:base="http://localhost:4000/cnn-and-rnn-simplified/">&lt;p&gt;Have you been feeling frustared about the complexity of deep learning terms like Convolutional Neural networks and Recurrent NNs?&lt;/p&gt;

&lt;p&gt;You have read countless articles with your goal in mind being to grasp and understand it finally - but it still doesnâ€™t click?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Donâ€™t worry. Youâ€™re not alone&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Iâ€™ve had my fair share of frustration. Trust me, reading all those blog posts that focuses on the calculations and theory part of it and still feeling lost has led me to study it through more abstract and practical ways.&lt;/p&gt;

&lt;p&gt;So even if you have found it difficult to assimilate, I want to clear out that situation and leave you a better deep learer with full understanding of CNNs and RNNs and excited to go forth and apply them in your projects. You can make it work, Iâ€™ll show you how.&lt;/p&gt;

&lt;h3&gt;Overview&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;How Deep Learning and CNNs Relate&lt;/li&gt;
  &lt;li&gt;CNNs Vs RNNs&lt;/li&gt;
  &lt;li&gt;How to Differentiate between their Uses&lt;/li&gt;
  &lt;li&gt;Applying them in your Projects&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I assume that you have an understanding about Deep Learning, and maybe even CNNs. You have heard or Deep learning Countless times and Convolutional Neural networks, but how do they relate? Have you ever asked yourself!. Answering that question is the key to simplifying the mystery that surrounds CNNs. Let me answer that questions in the easiest way possible.&lt;/p&gt;

&lt;h3&gt;What is Deep Learning and CNNs ( How do they relate?)&lt;/h3&gt;

&lt;p&gt;&lt;i&gt;&lt;b&gt;Deep learning&lt;/b&gt; is a subset of machine learning in artificial intelligence that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network.&lt;/i&gt; The diagram below explains Deep learning
&lt;img class=&quot;img-fluid&quot; alt=&quot;Deep Learning Diagram&quot; src=&quot;../assets\images\Blog\Blog-img\dl_explained.png&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;clear: both;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;i&gt;In deep learning, a &lt;b&gt;Convolutional Neural Network&lt;/b&gt; is a class of deep neural networks, most commonly applied to analyzing visual imagery. A Convolutional Neural Network (ConvNet/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Recurrent Neural Networks&lt;/b&gt; is a neural network designed for analyzing streams of data by means of hidden units where the output from previous step are fed as input to the current step.&lt;/p&gt;

&lt;p&gt;With the definitions above, you certainly have an idea of what Deep Learning and Convolutional Neural Networks are, but yet, this goes:&lt;/p&gt;

&lt;h4&gt;Simple and Detailed Explanation&lt;/h4&gt;

&lt;p&gt;&lt;b&gt;Deep learning&lt;/b&gt; is a field of study that introduces the concept of â€œartificial neural networksâ€ which works similarly to how the brain processes information and aims to recreate that in the processing of data by computers. In deep learning artificial neural networks are networks/ that are linked to eachother through out the system. they learn representations from data unsupervised i.e unlike regular traditional machine learning algorithm which requires an ,,, deep learning networks do not need the true output of the data to learn. These kind of data are unlabeled.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Convolutional Neural Network&lt;/b&gt; is a type of neural network that works specifically on image data to extract features that are unique in images, learns them through training/epochs and can carry out predictions on new unseen data using the representations learnt. This extraction of features work by passing a constant filter through the batch of images and applying a pooling layer onto the image which aims to highlight the features learnt. The convolution in CNNs means to convule/reduce the image to a smaller dimension and keep repeating this process.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Recurrent Neural Networks&lt;/b&gt; uses a sequential mode of working, in which the previous layer contains information that must be passed to the next layer in the step&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3&gt;How they Relate?&lt;/h3&gt;
&lt;p&gt;Deep Learning is a sub-field of Machine Learning and Artificial Intelligence. It uses deep neural networks to learn from unstructured data.&lt;/p&gt;

&lt;p&gt;Convolutional Neural netowrks(CNNs) are a class of Deep Neural networks(DNNs) that is used on â€œimage dataâ€ to learn features through downsizing, kernel passing, and pooling.&lt;/p&gt;

&lt;p&gt;On the other hand, a Recurrent neural Network is a class of DNNs that is usually used on text and sequential data for prediction the sequence of next few occurence. Examples of cases a RNN would be used: Chat/Text keyboards, Language Translation, Music Generation, etc. Now that you understand the overall concept of this subject, lets look at CNNs and RNNs on a deeper but straight forward level&lt;/p&gt;

&lt;h3&gt;CNN Vs RNN&lt;/h3&gt;

&lt;p&gt;&lt;i&gt;CNNs are used to extract features from unlabeled image data. If you are working on an image data of any sort, use CNN. CNNs are used to build models that cn take as input an image and output its class(Image Classification), Objects-in-image(Object detection) and even as far as thier positions in the image(Object Localization)&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;The structure of a CNN is basically a collection of neural networks, but what makes it different and unique to feature extraction in images is itâ€™s layers and thier functions. The CNN is made up of 2 important layers; Convolution and Max Pooling. It aslo has a kernel/filter which is for sliding through the image. So here is how it works:&lt;/p&gt;

&lt;p&gt;&lt;u&gt;Example&lt;/u&gt;
An Image of dimension 26X26 is first read and converted to 1s and 0s. Usually most images contains 3 channels which makes it a â€œcolorâ€ image. Grayscale Images has just 1 channel. A 3X3 kernel is slided through the image. It slides through with a stride that must be defined first. The stride determines how many columns is jumped over during the sliding. So if the &lt;b style=&quot;color: blue;&quot;&gt;STRIDE = 2&lt;/b&gt;, it moves over 2 columns and lands on the third one. The kernel contains all the numbers in the image enclosed as fits inside the filter. This extracted number is later applied to Activation RELU where all numbers less that 1 are rounded down to 0 and numbers greater than 1 are left as they are. This activation is processed on all the extracted kernels. The last stage is usually the Max pooling layer. The Max Pool layer looks at the box of number and simply picks the highest number in the group. This is done to all the other kernels and results in new sets of numbers with one from the previous kernel box. Now that the filter has been passed through, the Max Pool reveals the strongest lines in the image which can be noticed from the pattern in its numbers. The diagram below describe the Convolution process graphically:&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;img-fluid&quot; alt=&quot;Deep Learning Diagram&quot; src=&quot;../assets\images\Blog\Blog-img\feature_extract.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This explanation of CNNs shou;d give you a basic understanding of itâ€™s concept and workings, however for you to deeply â€œseeâ€ how it works, you must practice using it. Thatâ€™s a mistake I see most beginners make; you want to understand CNNs, youâ€™ve read articles on it but really never practically worked on it. The most basic project to start off with using CNN is &lt;b&gt;Image Classification&lt;/b&gt;. This project tutorial would guide you through &lt;a href=&quot;https://channelai.netlify.app/image-classification-api-in-tensorflow/&quot;&gt;How to Build an Image Classification Model and Web App with Tensorflow &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;RNN captures the sequential information present in the input data i.e. dependency between the words in the text while making predictions&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;img-fluid&quot; alt=&quot;Deep Learning Diagram&quot; src=&quot;https://cdn.analyticsvidhya.com/wp-content/uploads/2020/02/1d_POV7c8fzHbKuTgJzCxtA.gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sequential types of data require a different typr of neural network from CNN in order to be processed. Examples of sequential data are ; audio sound waves, single characters/words making up a text, projecting the movement of stock prices, genomic sequences and many other cases.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Example&lt;/b&gt;
This example below would explain what sequence/RNNs are, why they are used instead of CNN and in what kind of cases:&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;img-fluid&quot; alt=&quot;Deep Learning Diagram&quot; src=&quot;../assets\images\Blog\Blog-img\rnn-example.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You are given a sentence: This morning I took my cat for a walk. You want to build a deep Learning network to solve the task, but theres one underlying issue, â€œA feed-forward DNN can only take a fixed length input vectorâ€, and our problem here has a varying input length. Some sentence in the dataset are five words, others six and so on. Our ideal model needs to be able to handle variable lengths of inputs. There are three ways/ideas we can decide to take to solve this problem:&lt;/p&gt;

&lt;h4&gt;Idea 1&lt;/h4&gt;
&lt;p&gt;Hard code a certain length of input when defining the model hyper-pararmeters, i.e given a word: &lt;b&gt;I know how to speak French | fluently&lt;/b&gt;. If we choose a lenght of 3 for our model then, the sentence would be trimmed to; &lt;i&gt;to speak french&lt;/i&gt;. Using this method, we have an history problem:&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Problem 1&lt;/b&gt;Canâ€™t Model Long-Term Dependencies: In this problem, we need information from earler in the sentence but since our model is only using the 3 previous words, it cannot solve tasks like this.An example of a sentence with long-term dependencies: &lt;b&gt;France is where I grew up, but now I live in Boston. I speak fluent &lt;em&gt;__&lt;/em&gt; (French)&lt;/b&gt;&lt;/p&gt;

&lt;h4&gt;Idea 2&lt;/h4&gt;
&lt;p&gt;Use Entire sequence as sets of counts(also called bag of words). This means that a sentence would be represented by how much each words in it appears.&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;img-fluid&quot; alt=&quot;Deep Learning Diagram&quot; src=&quot;../assets\images\Blog\Blog-img\rnn_bagofwords.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this solution, we have a fixed length of vector which is all the possible words, and then we map a particular word to its representation in the â€œbag of wordsâ€. As you might have guessed, there is also a problem with this method&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Problem 2&lt;/b&gt;:Counts donâ€™t preserve order. We lose the sequential information in the sentence. take these two sentences for example:&lt;/p&gt;

&lt;p&gt;Sentence 1: The food was good, not bad at all
Sentence 2: The food was bad, not good at all&lt;/p&gt;

&lt;p&gt;Because they have the same representations, thses two sentences would mean the same thing although we both know they are opposites.&lt;/p&gt;

&lt;p&gt;The above situations are not ideal for sequential data types and information, that is where RNNs come in. Unlike the other solutions, RNNs achieve the following criteria:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Handle variable-legnth sequences&lt;/li&gt;
  &lt;li&gt;Track long-term dependencies&lt;/li&gt;
  &lt;li&gt;Main information about the order&lt;/li&gt;
  &lt;li&gt;Share parameters across the sequence&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;RNNs are networks that have loops in them that allows information to persist for a long period of time&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Logical example:&lt;/b&gt;An RNN receives and input x, propagates it through itâ€™s network and outputs a result, while also updating itâ€™s internal time-step which it passes this information to the next time-step. This is the process of sequential modeling, and RNNs approach to it. The RNN has three important cells which makes this process efficient and possible, they are called â€œgated cellsâ€.The rnn consists of three gated cells that process how information flows from one time-step to another;&lt;/p&gt;

&lt;p&gt;Forget: Forget gate forgets irrelevant information from the previous state&lt;/p&gt;

&lt;p&gt;Store: Store gate stores the relevant part of new information into the cell state&lt;/p&gt;

&lt;p&gt;Update: Update gate updates the cell using the previous and current information.&lt;/p&gt;

&lt;p&gt;Output: Output gate controls what information it receives is sent to the next time-step&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;img-fluid&quot; alt=&quot;Deep Learning Diagram&quot; src=&quot;../assets\images\Blog\Blog-img\lstm_gates.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This illustration Iâ€™m sure improves your understanding about Recurrent Neural Networks. And like CNNs, it would be more profitable for you to have some practice with it to sharpen your understanding. This tensorflow article works you through in an intuitive way &lt;a href=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now that you know individually what CNNs and RNNs are, how they work and thier structure. Letâ€™s look at the difference between the two. This would make it easier for you to easily indentify when to apply them&lt;/p&gt;

&lt;h3&gt;Difference Between CNNs and RNNs&lt;/h3&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;th&gt;CNN&lt;/th&gt;
    &lt;th&gt;RNN&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;CNNs are ideal for images and video processing.&lt;/td&gt;
    &lt;td&gt;RNNs are ideal for text and speech analysis.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;This network takes fixed size inputs and generates fixed size outputs.&lt;/td&gt;
    &lt;td&gt;RNN can handle arbitrary input/output lengths.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;CNN is considered to be more powerful than RNN.&lt;/td&gt;
    &lt;td&gt;RNN includes less feature compatibility when compared to CNN.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;It is suitable for spatial data such as images&lt;/td&gt;
    &lt;td&gt;RNN is suitable for temporal data, also called sequential data.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;CNN is a type of feed-forward artificial neural network with variations of multilayer perceptrons designed to use minimal amounts of preprocessing.&lt;/td&gt;
    &lt;td&gt;RNN unlike feed forward neural networks - can use their internal memory to process arbitrary sequences of inputs.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;CNNs use connectivity pattern between the neurons. This is inspired by the organization of the animal visual cortex, whose individual neurons are arranged in such a way that they respond to overlapping regions tilting the visual field.&lt;/td&gt;
    &lt;td&gt;Recurrent neural networks use time-series information - what a user spoke last will impact what he/she will speak next.&lt;/td&gt;
  &lt;/tr&gt;&lt;/table&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;These diffeences makes it very easy for you to remeber the properties and features of either CNn or RNN. You now have a broad understanding of these terms but you donâ€™t still know how to apply them in your projects. Iâ€™ll show you how in the next section.&lt;/p&gt;

&lt;h3&gt;Practical Use of CNNs and RNNs&lt;/h3&gt;

&lt;p&gt;&lt;u&gt;CNNs&lt;/u&gt; are use mainly for datasets and tasks involving Image manipulation. These kind of tasks include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Image Classification: Classifying an image into a class or group. The ImageNet dataset can be use for Image classification&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Object Detection: Classifying groups of objects in an Image. The Coco Dataset is used for object detection&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Object Localization: Locating of mapping the Locations of objects in an Image. The RCNN library can be used on any dataset with Images and annotation files.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Style Transfer: The transferring of the style in one image to another without damaging the latter and while retaining its information.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are a lot more applications of CNNs mainly in the field of Computer Vision and while I have just showed you some simple projects, know that all these are built at production level with more advanced systems into the Large World changing technologies we see today like Drone Navigation, and Autonomous Vehicles.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;u&gt;RNNs&lt;/u&gt; are also used in many industries today and while that holds true, it is fair to say that CNNs are more widely used. Sone applications of RNns includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Text Translation: Mapping English words in a sequence and translating each word into a different lanuguage like Spanish or French.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Music Generation: RNNs being superior in sequential data, gives them ability to generate chords of music after being trained on a latge collection of music data. Usually, an RNn would generate music similar to the one it has been trained on&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;End Notes&lt;/h3&gt;
&lt;p&gt;Now the end of this comprehensive article, I mostly hope that it has achieved itâ€™s purpose with you; To simplify the subject of RNN and CNN without much alogrithm and theory. Itâ€™s a really long article and you might find after sometime, you would need to refer back to it for some details. that is infact highly recommended for you to grasp the main concepts effectively. Have a question or an area you need some clarity in? Please leave me a comment and I will be glad to answer it.&lt;/p&gt;</content><author><name>Chel</name></author><summary type="html">Have you been feeling frustared about the complexity of deep learning terms like Convolutional Neural networks and Recurrent NNs?</summary></entry><entry><title type="html">9 Key Skills Every Deep Learning Practitioner Should Have (How many in the List do you Have?)</title><link href="http://localhost:4000/key-deep-learning-skills-beginner/" rel="alternate" type="text/html" title="9 Key Skills Every Deep Learning Practitioner Should Have (How many in the List do you Have?)" /><published>2020-10-09T00:00:00+01:00</published><updated>2020-10-09T00:00:00+01:00</updated><id>http://localhost:4000/key%20deep%20learning%20skills%20beginner</id><content type="html" xml:base="http://localhost:4000/key-deep-learning-skills-beginner/">&lt;p&gt;Are there specific skills important in deep learning?. You shouldnâ€™t be surprised if you ask yourself this question. At a time or another, all budding deep learner ask themselves the same question. Are you a deep learner that flows with the tides that comes along with projects or are you set on building important skills that can make you more effective. In this article, I share 9 skills backed by stactics that guarantees exponential growth in your deep learning career.&lt;/p&gt;

&lt;p&gt;Skills are very important. In some cases, more important than others. They help us to go from zero to one in any new field of our interest. This being the case, why do we have learners that never really attain the peaks of their careers since the skills necessary for success is available for all to master?. Could it be that they are learning the &lt;i&gt;wrong&lt;/i&gt; skills, learning it the wrong way or more critical, having a skewed perspective to skill gathering.&lt;/p&gt;

&lt;p&gt;What then is the correct answer to successful skill gathering for deep learning?. The answer, as you might have guessed lies in the tactful combinations of several skills. Deep learning is a broad field encompassing a number of skills we need to succeed. There is no one-size-fits-all approach. But there are a few key skills you can pick up to ensure that youâ€™ll become a good deep learning practitioner.&lt;/p&gt;

&lt;p&gt;The deep learning domain has shifted from being just a degree focused industry to a skill-based industry. Relying on just a degree to become a professional for whahtever sub-domain you choose is now a thing of the past.
Education lands you a job but skills scale up your prospects for growth. In this article, we are going to explore the most important skills required by a deep learner - professional or newcomer.&lt;/p&gt;

&lt;h3&gt;Overview&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Difference between a Deep Learner and a Deep Learning Practitioner&lt;/li&gt;
  &lt;li&gt;Technical Skills in Deep Learning
    &lt;ul&gt;
      &lt;li&gt;Data Preparation&lt;/li&gt;
      &lt;li&gt;Data Visualization&lt;/li&gt;
      &lt;li&gt;Functional Programming with Python or R&lt;/li&gt;
      &lt;li&gt;Deep Learning Frameworks and Libraries&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Soft Skills in Deep Learning
    &lt;ul&gt;
      &lt;li&gt;Problem Solving&lt;/li&gt;
      &lt;li&gt;Communication&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Difference Between a Deep Learner and a Deep Learning Practitioner&lt;/h3&gt;

&lt;p&gt;Often I have seen how people use the above 2 terms in a similar context. When people say â€œI want to be a Deep learnerâ€, In reality and in their heads they mean a Deep Learning Practitioner. So whatâ€™s the difference between the two terms? Letâ€™s clarify.&lt;/p&gt;

&lt;p&gt;A &lt;b&gt;Deep Learner&lt;/b&gt; is more on the theoretical side, refers to a person that is of course enthusiastic about Deep learning as a topic and focuses on learning about itâ€™s concepts, theories and advancement from research papers, blogs and personal research. It is not necessarily a person that puts theory into practise aimed at developing models that solves a problem.&lt;/p&gt;

&lt;p&gt;A good and popular example of a deep learner is &lt;a href=&quot;https://en.wikipedia.org/wiki/Geoffrey_Hinton&quot;&gt;Geoffery Hinton&lt;/a&gt;. You can read about him on wikipedia but basically he is a computer scientist and professor at the University of Toronto. He has brought about tremendous advancement to the fields of Deep learning and computer vision through notable research papers and milestones.&lt;/p&gt;

&lt;p&gt;A &lt;b&gt;Deep Learning Practitioner&lt;/b&gt; focuses more on practical, technical, analysis, and data side of deep learning. They aim to solve real-world problems by applying Artificial Neural Networks on data to learn functions, build models and deploy for scalable predictions or use-cases. In short, they are the one that&lt;/p&gt;

&lt;p&gt;Also a good example in this case woule be Andrew Ng who is less focused on publishing papers(that ofcourse provides groundbreaking ideas which community can build on) and more on building solutions to real-world problems.&lt;/p&gt;

&lt;p&gt;In the following sections, we are going to read about the key skills a Deep learning Practitioner should have&lt;/p&gt;

&lt;h3&gt;Technical Skills for Deep Learning Practitioners in 2020&lt;/h3&gt;

&lt;h4 style=&quot;color: blue;&quot;&gt;Data Preparation&lt;/h4&gt;

&lt;p&gt;Preparing data may be the most important part of a predictive modeling project and the most time-consuming, although it seems to be the least discussed. The ability to prepare data effectively for use in your projects is as essential as the steps of training the models themselves. Although I know training can feel a lot more fun, afterall thatâ€™s where you derive feelings of achievement and effort realised but donâ€™t be fooled to think training is a more important step than preparation. If weâ€™re all being honest, it takes way lesser effort at training stage and if you are using google colab for your work, then thatâ€™s just unchallenging to say the least.&lt;/p&gt;

&lt;p&gt;Data preparation is usually the first skills needed in any deep learning project. The truth is that, lack of this important skill would reduce your pace in projects completion - every single time. Why? Data comes in different ways and formats. If you choose to rely on stack overflow every time for different and unique data arrangements, they wouldnâ€™t always deliver. You would end up spending valuable time that could have been channeled to advancement in your project.&lt;/p&gt;

&lt;p&gt;To give you a sense of the full context of what I mean by data preparation:&lt;/p&gt;

&lt;p&gt;&lt;i&gt;Data preparation is the process of cleaning and transforming raw data prior to processing and analysis. It is an important step prior to processing and often involves reformatting data, making corrections to data and the combining of data sets to enrich data.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;Random data are never collected in a format or situations that appeal to the analyst that work with them - Except in some specialised cases of course!&lt;/p&gt;

&lt;p&gt;As a useful resource and getting started path for you, below is a list of the 4 areas that will get you started and productive with Data Preparation in Deep Learning:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;Data Cleaning&lt;/b&gt;: Identifying and correcting mistakes or errors in the data. It is hard to find a huge data collection that isnâ€™t messy - containing error and missing values. The easiest tool to use to combat this issue is &lt;a href=&quot;&quot;&gt;Pandas&lt;/a&gt;. Whatâ€™s better is that you can install/import this python library into almost any environment - locally or online like colab. Pandas is a very easy library to master, for the purpose of data cleaning you can get helpful tutorials on their website. You would learn simple to advanced techniques of how to find missing vales, fill them or remove data rows with erros.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;Feature Selection&lt;/b&gt;: Identifying those input variables that are most relevant to the task. Preparing data for use means to polish it into a state that you can readily use in your modeling project. Feature Selection is step in the data preparation process where you filter and use specific data of your choice.&lt;/p&gt;

    &lt;p&gt;Recursive Feature Elimination, or RFE for short, is a popular feature selection algorithm. The scikit-learn Python machine learning library provides an implementation of RFE for machine learning. To get started, visit the scikit-learn docs for RFE where you can learn process that can be repeated for your projects.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;b&gt;Normalization&lt;/b&gt;: Changing the scale or distribution of variables. One of the most popular techniques for scaling numerical data prior to modeling is normalization. Normalization scales each input variable separately to the range 0-1, which is the range for floating-point values where we have the most precision. This can be done very easily on tensorflow with keras and scikit-learn using the object MinMaxScaler, follow either link for &lt;a href=&quot;https://www.tensorflow.org/tutorials/keras/classification#preprocess_the_data&quot;&gt;tensorflow&lt;/a&gt; or for &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html&quot;&gt;scikit-learn&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;b&gt;Data Transforms&lt;/b&gt;: Transform Data into other Feature types including One-Hot and categorical Encoding . In some of your Deep Learning projects your models may require all input and output variables to exist in a transform different from itâ€™s source. This means that you would need to transform in a manner that is useable for training. Popular DL frameworks has libraries that make this process effortless. 
As before, practicing on tutorials covering this subject would build up the skills for future use. This resource on tensorflow explains the Different &lt;a href=&quot;https://www.tensorflow.org/tutorials/structured_data/feature_columns&quot;&gt;Feature Types&lt;/a&gt; and how to convert/map your values to a specific type. scikit-learn also has a similar tutorial covering &lt;a href=&quot;https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html?highlight=feature%20types&quot;&gt;Column Transformer with Mixed Types&lt;/a&gt;. depending on the platform you use up till now, make a choice&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 style=&quot;color: blue;&quot;&gt;Data Visualization&lt;/h4&gt;

&lt;p&gt;Data is visualized in every industry in the 21st century.Data visualization is the act of taking information (data) and placing it into a visual context, such as a map, charts and graph. Data visualizations make big and small data easier for the human brain to understand, and visualization also makes it easier to detect patterns, trends, and outliers in groups of data. Usually, visualization makes it easier to understand the dataset you are working on and to work out the necessary steps it would take you for modeling.&lt;/p&gt;

&lt;p&gt;In Python, there are libraries that are widely used for visualization like &lt;a href=&quot;https://matplotlib.org/&quot;&gt;Matplotlib&lt;/a&gt;. To use this tool effectively for various data types and sizes, you would need learn how it works form tutorials you can find on the official page.&lt;/p&gt;

&lt;p&gt;Matplotlib is a great tool when you just want to visualize data on a notebook conveniently but for more interactive displays and plots, some more platforms should be used:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.tableau.com/&quot;&gt;Tableau&lt;/a&gt;: By far the most popular, Tableau is a visual analytics platform transforming the way we use data to solve problems
&lt;img src=&quot;https://www.tableau.com/themes/custom/tableau_www/logo.png&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://powerbi.microsoft.com/en-us/&quot;&gt;Power BI&lt;/a&gt;: Power BI is a business analytics service by Microsoft. It aims to provide interactive visualizations and business intelligence capabilities with an interface simple enough for end users to create their own reports and dashboards. To use it for Deep Learning and Machine Learning, you would need to import a few libraries like PyCaret. To get stated in this, follow this guide by towardsdatascience &lt;a href=&quot;https://towardsdatascience.com/machine-learning-in-power-bi-using-pycaret-34307f09394a&quot;&gt;Machine Learning in PowerBI using PyCaret&lt;/a&gt;
&lt;img src=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 style=&quot;color: blue;&quot;&gt;Functional Programming with Python&lt;/h4&gt;
&lt;p&gt;In Deep Learning fields, the process of cleaning, preparing, processing, visualizing and others usually needs you to have intermediate to advanced python skills for writing functions that can execute multiple steps at once therby saving time. For example: You want to build an image classify that differentiates between horses and humans and returns the correct answer. The dataset available for training happened to be coupled together i.e the both horses and humans pictures are in the same folder. Luckily, the names of the files are correctly named as either horse/humans. You know you can use naming as a way to separate the files into four folders: train_horses, train_humans, test_horses and test_humans. I write this example from experience. I knew python way back then, but really just the beginner side of it. There are a few libraries you need to accomplish that task. thatâ€™s why Python or R language is a must have for Deep Learning, else u would waste valuable time searching online for a solution. Python is one of the easiest languages to learn and there are hundreds of sites you can learn it by practising on. To learn functional programming, use the resources you can find in these platforms:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.codecademy.com/&quot;&gt;Codecademy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.w3schools.com/python/&quot;&gt;w3 schools&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.geeksforgeeks.org/python-programming-language/&quot;&gt;Geeks for Geeks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.pluralsight.com/browse/software-development/python&quot;&gt;Pluralsight&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.learnpython.org/&quot;&gt;Python Official&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 style=&quot;color: blue;&quot;&gt;Deep Learning Frameworks and Libraries&lt;/h4&gt;

&lt;p&gt;Skills in using Deep Learning frameworks is a must have for any DL Practitioner. They make your work easier and more productive. Some Machine Learning algorithms are ofcourse still used by some engineers but now in every case would they be the best option to go for. In fact, Tensorflow the most popular tools for Machine Learning Enginners is a DL Framework. To build up skills in any framwork is now a very easy ordeal. Every tool now has an immnense library of tutorials to take you from beginner to expert level. Some of the most widely used frameworks and libraries are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;Tensorflow&lt;/a&gt;: TensorFlow is a free and open-source software library for dataflow and differentiable programming across a range of tasks. It is a symbolic math library, and is also used for machine learning applications such as neural networks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://keras.io/&quot;&gt;Keras&lt;/a&gt;: Keras is an open-source library that provides a Python interface for artificial neural networks. Keras acts as an interface for the TensorFlow library. Up until version 2.3 Keras supported multiple backends, including TensorFlow, Microsoft Cognitive Toolkit, R, Theano, and PlaidML.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://scikit-learn.org/&quot;&gt;Scikit-Learn&lt;/a&gt;: Scikit-learn (formerly scikits.learn and also known as sklearn) is a free software machine learning library for the Python programming language.[3] It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://pytorch.org/&quot;&gt;Pytorch&lt;/a&gt;: PyTorch is an open source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, primarily developed by Facebookâ€™s AI Research lab.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://numpy.org/&quot;&gt;Numpy&lt;/a&gt;: NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Soft Skills Required for Deep Learning&lt;/h3&gt;
&lt;p&gt;There are a number of qualitative skills that make a technically efficient deep learning candidate. The following are some of the prominent soft skills that a deep learning practitioner should have.&lt;/p&gt;

&lt;h4 style=&quot;color: blue;&quot;&gt;Problem Solving&lt;/h4&gt;

&lt;p&gt;Problem Solving in Deep Learning and ML requires the person to logically apply a combination of thoughts, processes, and actions to effectively and correctly reach the end goal they want.&lt;/p&gt;

&lt;p&gt;A good problem solver tackles a task by making less assumptions and instead researching more to find out what works. Before you can solve a problem, you must first understand it in your own language. How to break it down to understand might be the problem for most of us. There are a few qualities and behaviours you can employ that is guaranteed to help you comprehend and interpret any Deep Learning task:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Eliminate fear: The first step is to keep a neutral and positive mind. Thinking or feeling that the task is too tough for you is killing the process of problem-solving even before starting. To eliminate fear, simply believe that you can do it, you may be a beginner but know this: Every expert was once a beginner.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Think Accurately: While reading the problem to be solved, try to see the words in your mindâ€™s eye and create patterns of what they mean as you read. As you do this, surely you would find tasks that you totally do not understand. Which takes us to the next step&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Reserach: The internet is full of resources in whatever subject you are researching. While browsing how-tos and guides make sure to not find the exact task you are working on but unstead something similar that sheds light on your task and can be employed in solving it.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With these four steps you can iterate through problem discovery and completion. On the process, you may discover other process that works best for you. Do ensure to practise it in your work, if it inproves effectiveness and productivity. At the core of it, every Deep Learning Practitioner requires Problem Solving ability&lt;/p&gt;

&lt;h4 style=&quot;color: blue;&quot;&gt;Application of Deepl-learning&lt;/h4&gt;
&lt;p&gt;Solving problems is good, building an application with your Deep learning model is Priceless. If you want to be a deep learning practitioner, then you must know that applying model in a real-world case is essential, not just that, this can benefit you in a number of ways:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Interest to prospective Employers: You probably want to be noticed and get a full-time job in a deep learning field. What you should know is that, itâ€™s better to show employers a web application that works thatn to show them notebooks where you built your model. Remember that, the model doesnâ€™t have to be advanced, not at all, that would come naturally if you keep up with your goals.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Learning process for you: Learning Model Deployment earlier on would give you a very important skill. Learning about model deployment is a process 80% of beginners ignore, but if you master the process you give yourself an edge among the crowd. Deployment can be done in different ways; cloud, self-hosting, third-party services etc. this post on deployment, &lt;a href=&quot;&quot;&gt;&lt;/a&gt; would teach you everything you need to start.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;End Notes&lt;/h3&gt;
&lt;p&gt;To conclude, in this article, we understood the skills required for a deep learning practitioner. Also, we explained in brief, the difference between a deep learner and a deep learning practitioner and why you should strive to achieve the latter.
Have other skills in mind? What skills do you think is the best for aspiring deep learning practitioners? Let us know in the comments below.&lt;/p&gt;</content><author><name>Chel</name></author><summary type="html">Are there specific skills important in deep learning?. You shouldnâ€™t be surprised if you ask yourself this question. At a time or another, all budding deep learner ask themselves the same question. Are you a deep learner that flows with the tides that comes along with projects or are you set on building important skills that can make you more effective. In this article, I share 9 skills backed by stactics that guarantees exponential growth in your deep learning career.</summary></entry><entry><title type="html">Starting your First Computer Vision Project? Here are 10 Things you Must Absolutely know</title><link href="http://localhost:4000/key-points-before-starting-computer-vision/" rel="alternate" type="text/html" title="Starting your First Computer Vision Project? Here are 10 Things you Must Absolutely know" /><published>2020-09-21T00:00:00+01:00</published><updated>2020-09-21T00:00:00+01:00</updated><id>http://localhost:4000/key%20points%20before%20starting%20computer%20vision</id><content type="html" xml:base="http://localhost:4000/key-points-before-starting-computer-vision/">&lt;p&gt;Can you imagine navigating through an unknown city without Google Maps?It is usually a tough and tiring ordeal, all paths seems to lead to nothing and at the end you even ask yourself why you took that path in the first place.&lt;/p&gt;

&lt;p&gt;Thatâ€™s often what the first Computer Vision project feels like. I can personally attest to this because I have been through it and now I know some real facts of what should and not be done.&lt;/p&gt;

&lt;p&gt;Learning the basics of CV and building a Computer Vision Model in tensorflow is great - but if you think that alone would land you a great job and carry you on to success, youâ€™ll be in for a big shock. In reality, it just isnâ€™t. For me, this reality hit home when I began building models on after the other, on colab solving projects on Kaggle, blog tutorials and official tensorflow website. There were other tons of things which were just as important, such as data collection, cleaning, exploration etc.&lt;/p&gt;

&lt;p&gt;A few things I realized - problem solving skills, definite purpose, imagination, creativity are more helpful than mastering model building algorithms and tools. Trust me, this is way important than you might think.&lt;/p&gt;

&lt;p&gt;In this article, I will be sharing 10 key points that I wish I knew when I started my Computer Vision journey, I hope this will help you out in your own CV journey too.&lt;/p&gt;

&lt;h3&gt;Stick to a Computer Vision Niche at First&lt;/h3&gt;
&lt;p&gt;As riduculous as it sounds, the firld of Computer Vision has niches. How can something so broad not have a niche?. If you wanna see for yourself just how, go to google.com and serach for the term â€œcomputer visionâ€, there are almost 2 billion results for it. &lt;br /&gt;
&lt;img class=&quot;img-fluid&quot; alt=&quot;computer vision search results image on google&quot; width=&quot;50%&quot; height=&quot;50%&quot; src=&quot;/assets/images/Blog/Blog-img/cv-searchresults.webp&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I canâ€™t stress this enough, probably why it is placed as the first key point. When learning any new skill, one thing that you absolutely must try to avoid - Indecision. As a beginner, you are excited about this skill and want to try your hand in different paths thereby inviting time wasting which comes from Indecision. Trust me, I was once preparing for my First CV project so I know how it can be. What you want to do is stream line your thoughts to specific things only because your mind would be divided among different things to try. A simple question you can ask yourself: &lt;b&gt;What should you start with&lt;/b&gt;, and my answer to that is â€œ&lt;i&gt;start from the basics&lt;/i&gt;â€. It significantly helps you on the long run. Almost every accomplished CV expert will give you the same advice even those that took a different path but later realized â€œbasics are the roots and are very essentialâ€. When you start from the basics you gather the basic steps,libraries,algorithm(which might take time), but afterwards you gain a momentum thatâ€™s double those that began straightaway pulling projects from github. A very good place to start that spells â€œbasicâ€ is Image Classification in Computer Vision. This article would get you started &lt;a href=&quot;https://channelai.netlify.app/image-classification-api-in-tensorflow/&quot;&gt;Building an Image Classification Model with Keras&lt;/a&gt;.&lt;/p&gt;

&lt;h4&gt;How to decide between the niches&lt;/h4&gt;
&lt;p&gt;I mentioned niches in Computer Vision, you might be wondering what they are. Here I would give some examples on how Computer Vision is segmented today.
&lt;b&gt;Note:&lt;/b&gt; Listed here are just a few from my knowledge that is, more experienced practitionerws would be able provide detailed examples and instances.
Some CV Niches are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Classification projects like - Image Classification, Image Segmentation, Object Detection, Text Detection&lt;/li&gt;
  &lt;li&gt;Synthesis projects like - Style Transfer, Image Colorization, GANs&lt;/li&gt;
  &lt;li&gt;Real-time Prediction like - Image Captioning, Life face detection, Video recognition and analysis, Autonomous Vehicles.
Itâ€™s obvious that they are listed in increasing other of complexity. Here you should start with classification problems which are generally the easiest, understanding them extensively before moving to others. This would make your confidence grow as you complete simple projects, feeling good with your new skill and ability would make you love CV and push on to bigger projects. Unlike when jumping between these niches which causes confusion, indecision and generally kills motivation. Remember that you are just getting started, not only do you have the right, itâ€™s necessary to start from the basic projects.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Knowledge of Computer Vision tools is great; Ability to break down data problems is priceless&lt;/h3&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;Computer vision tools will come and go but the basics will stick forever&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;Here is a very crucial step that beginners ignore without even knowing it. I have seen so many folks new to CV rush off to &lt;b&gt;master&lt;/b&gt; specific tools and softwares with the hope that it would make them more effective and work smarter. That, is never the case. What is Effectiveness to begin?. Effectiveness is doing more in lesser time. Tools like Open CV, Matlab, CUDA, Simple CV etc are designed to make you work faster. How do you put them to good use if you donâ€™t even know how to break a problem into actinable steps. Using this tools is great, but NOT at the expense of you learning problem solving skills and business intelligence. You can spend 2 whole months getting familair with an advanced CV tool. In reality, you donâ€™t even need it. Certainly not at beginner stage.
As an aspiring Computer Vision Enginner you have to create solutions by solving  real-world problems.&lt;/p&gt;

&lt;p&gt;I will show you how you can go about developing the ability to break down data problems: You would need a lot of confidence at first, with time - you thrive in it. The best way to build these skills is by solving real world problems from communities like &lt;a href=&quot;https://www.kaggle.com/&quot;&gt;kaggle&lt;/a&gt;, &lt;a href=&quot;https://zindi.africa/hackathons&quot;&gt;zindi&lt;/a&gt; and other deep learning hackathons you can find online.&lt;/p&gt;

&lt;p&gt;Here I will give you a working cheatsheet you can use to learn, build and master problem-solving skills in Computer Vision (just follow the steps below):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Open up Tensorflow Google Colab &lt;a href=&quot;https://colab.research.google.com/drive/1nm_ahlpLXgANRt-8HMrOsdNPh5GDWlVz&quot;&gt;here&lt;/a&gt; and bookmark an empty notebook&lt;/li&gt;
  &lt;li&gt;Creat an acct on kaggle or login in if you already have one &lt;a href=&quot;https://www.kaggle.com/data-science-for-good-kiva-crowdfunding-signup&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Go to your acct dashboard on kaggle and download your unique kaggle id to your computer. This is a json file&lt;/li&gt;
  &lt;li&gt;Go to the datasets section on kaggle, use the filter to narrow competitions down to â€œcomputer visionâ€ &amp;amp; â€œbeginnerâ€&lt;/li&gt;
  &lt;li&gt;When you find a project you like, open it up. Read the information of the project. Go to the data section and scroll down to data explorer to visualize the files. In the top right of the page,  click the three dot icon you see and tap â€œCopy API commandâ€. You will use this to download the project directly on google colab.&lt;/li&gt;
  &lt;li&gt;Next copy these lines of code below. You would need to do two things - Change the url-of-the-project to the one you copied, - upload your own kaggle user id you previously downloaded when you are prompted for it.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;code to set up kaggle project on colab&lt;/h5&gt;
&lt;pre&gt;&lt;code class=&quot;css&quot;&gt;
# upload your kaggle.json file you downloaded
from google.colab import files
files.upload()
# this code will move it to a new directory
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json
# download ur dataset with this code( replace with the command you copied)
!kaggle competitions download -c titanic
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
  &lt;li&gt;After completing the above steps the next step is for you to break down the problem, find resources and solve it. I recommend that before attempting a project from kaggle, you should have had experience using colab and have completed the &lt;a href=&quot;&quot;&gt;Image classification beginner project&lt;/a&gt;. You can always come back to this article to begin, but do all these in the smallest possible time available to you. Donâ€™t creat time for procrastination.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By following these guidelines, you would no doubt develop problem-solving skills which is much needed to secure any CV job or becoming great at Computer Vision.&lt;/p&gt;

&lt;h3&gt;Model Deployment is key - Learn Software Enginnering&lt;/h3&gt;
&lt;p&gt;One thing i see a lot of beginners do is avoid deployments of models with the excuse that it isnâ€™t important for them. What is Computer Vision without models we can put to use, honestly very irrelevant. What is the need of bothering about CV at all if you decide to not focus on the product - Deployment and Predictions. You build a working model on a jupyter notebook or colab, but it remains there waiting to be â€œreadâ€ and maybe referred to later when you need some code resources in it. That is not all the use case of a model. Models are meant to solve problems. Now though, let me tell you something else I have observed - Some folks choose to ignore deployment process because they are scared of the task â€œthinkingâ€ it just way too much work and also for more professional individuals.&lt;/p&gt;

&lt;p&gt;I would assure you now that - it really is not hard to deploy a CV model and make predictions with it, how do i know?, because I deployed a model on Image classification using tensorflow serving. So starting your first project in computer vision, keep model deployment in mind as an important step for success in CV. It is helpfulm to start it now, learn the ropes, so it wonâ€™t be a total new experience when you need it most. This article will teach you everything you need to know about model deployment &lt;a href=&quot;https://channelai.netlify.app/scalable-predictions-with-deep-learning-keras-model/&quot;&gt;How to make Scalale Predictions with a Deep Learning Model Built with Keras&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;Appreciate softwares with a simple learning curve&lt;/h3&gt;
&lt;p&gt;In any field of study two different softwares performing closely related tasks for users are usually cause for lots of argument comparation and reviews. In a good way, this is helpful to newcomers as it helps them make a choice of software that suits their needs closest and on ther other side of the coin, this brings about confusion and indecision. When I first statrted learning about CV, I was contemplating the ML framework to use for building models. I checked reviews, and honestly why it is always hard to make a singular choice and stand by it is because, Softwares have features they beat their competitor at and those they are lacking at. We want all the acctractive features to be present in one tools or software and usually this is never the case. As a beginner, a key guide to choosing the software you would employ in development process is to find the one with the easiet learning curve, not that with the best features - at first of course. You need a tool that you can assimilate in the shortest time possible, atool that doesnâ€™t take days to set up, in short a simple tool providing it allows for the basics of what you are searching for.&lt;/p&gt;

&lt;p&gt;In Computer Vision and Deep Learning in general, we have a bunch of softwares. Concerning DL frameworks, people would argue for and against Tensorflow having the easiest interface and learning curve when compared to other frameworks like pytorch, scikit-learn, Theano, Apache Spark etc. But if Iâ€™m being absolutely earnest, Tensorflow is the framework every beginner should start with for these reasons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It has a large community of support in case you run into issues when developing&lt;/li&gt;
  &lt;li&gt;It has a dedicated tutorial library for folks at any level of expertise right from â€œyour first dl project(usally classifying hand-written digitsâ€. You donâ€™t need to look for Tensorflow projects online, you can start right from the website. And this is filled with all the important dl projects till date&lt;/li&gt;
  &lt;li&gt;It has the easiest and fastest deployment methods. Tensorflow has a few tools you can use to push your complete model for real-world use. Examples are: Tensorflow Extended, Tensorflow.js, Tensorflow Lite. With these tools you can deploy to any platform you choose like - web browser, mobile, or back-end server.&lt;/li&gt;
  &lt;li&gt;Itâ€™s methods and functions are very easy to pick up by a total beginner. For a fact, I learnt the basics of Tensorflow in just a week following along with a &lt;a href=&quot;https://www.coursera.org/learn/introduction-tensorflow/home/&quot;&gt;Introduction to Tensorflow for AI, ML and DL&lt;/a&gt; course on Coursera.&lt;/li&gt;
  &lt;li&gt;You donâ€™t necessarily need to install it to use it first. You can start building a model directly on colab, with all tools and libraries available and without any hassels.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Make Consistency your Watch-Word&lt;/h3&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;Consistency creates habit. Once something has become an habit, you do it naturally and it rarely every leaves.&lt;/p&gt;

&lt;p&gt;First and fore most, you should love to work in Computer Vision field. Without this love, we canâ€™t even start talking consistency. If this is the case, then I cannot stress this enough. I you want to take just ONE guideline from all the key points, let it be this one. This just tells you how important it is. You canâ€™t trade consistency for anything - even money. You want to know why I picked this? Because by applying consistency to your work in CV, you would sooner or later figure out the other helpful tips I have given you. All these tips are written to make you reading it right now understand them â€œearly onâ€ not that you canâ€™t figure them out for yourself. In other words, Iâ€™m trying to save you all the time you will spend by choosing the wrong paths which is a common thing for beginners. Back to consistency, what I mean is this: If you work and practice Computer Vision tutorials and projects every other day, you can achieve in months what others strived years to achieve because of their lack of consistency. I canâ€™t really do much to help you get to that level of consistency that is desirable, itâ€™s more of a personal endeavor. What I can make you undertsand is this: If you make becoming an expert in Computer Vision you Definite Chief Aim(what you want to achieve everyday more than anything), you would attain the greatest heights possible no matter how low you might start. Understand that statement, believe in it, and thatâ€™s the best assistance I can render in this case.&lt;/p&gt;

&lt;p&gt;That brings us to the end of this article on guidelines to help you take a start in the field of CV, I hope most that you find this helpful on the long run. Naturally questions would popped in your mind as you have read this comprehensive guidelines, know that your comments are very welcomed and if you please let me know if you found this helpful and what your next step would be in your quest to master Computer Vision. Thanks, Chel_&lt;/p&gt;</content><author><name>Chel</name></author><summary type="html">Can you imagine navigating through an unknown city without Google Maps?It is usually a tough and tiring ordeal, all paths seems to lead to nothing and at the end you even ask yourself why you took that path in the first place.</summary></entry><entry><title type="html">How to make Scalale Predictions with a Deep Learning Model Built with Keras</title><link href="http://localhost:4000/scalable-predictions-with-deep-learning-keras-model/" rel="alternate" type="text/html" title="How to make Scalale Predictions with a Deep Learning Model Built with Keras" /><published>2020-09-17T00:00:00+01:00</published><updated>2020-09-17T00:00:00+01:00</updated><id>http://localhost:4000/scalable%20predictions%20with%20deep%20learning%20keras%20model</id><content type="html" xml:base="http://localhost:4000/scalable-predictions-with-deep-learning-keras-model/">&lt;p&gt;Buiding Models is exciting, putting them to use can be very brain-tasking. For people who have mastered model deployment, usually this involves just a few tweaks to the code to make it work for a new problem and situation. For beginners like you and me, the task of scaling models for prediction for use in different environment requires a lot of information gathering, software set-up and a little bit of technical know-how. In this article, my aim is to simplify this process for you and show you the methods and steps you can immediately put to use in your projects.&lt;/p&gt;

&lt;p&gt;These are the steps guiding us through this article:&lt;/p&gt;

&lt;h3&gt;Overview&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Introduction&lt;/li&gt;
  &lt;li&gt;Buidling Deep Learning Models in Keras&lt;/li&gt;
  &lt;li&gt;Making Predictions: Methods to Utilize a Model for Predictions&lt;/li&gt;
  &lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;As a beginner, usually you spend most of your time building models by leveraging blog posts tutorials, &lt;a href=&quot;http://kaggle.com/&quot;&gt;kaggle data-science&lt;/a&gt; community or follow-up from courses you take. At least this was what most of my time has been spent doing.&lt;/p&gt;

&lt;p&gt;The beauty about model deploment and prediction is that you  can see how your modelperforms in a real-world situation which is an important step that develops a very useful skill every Machine Learning Enginner should possess. No-one wants an enginner that cannot solve problems with their models because it remains sitting on google colab or on thier machine.&lt;/p&gt;

&lt;p&gt;There is one quality every smart ML Enginner or Data-Scientist possesses and that is â€œthe ability to know the best procedure for every unique task or problemâ€. I say this because, in deep learning no two project is the exact same thing, there are always a few tweaks that must be made to fit a certain unique problem. In model deployment, this same fact holds true and later in the article, an example would be given that portrays how two problems has different routes that ensures the best results. That said, it is necessary to understand the various methods of how you can make scalable predictions from your deep learning models.&lt;/p&gt;

&lt;h3&gt;Buidling Deep Learning Models in Keras&lt;/h3&gt;
&lt;p&gt;It is assumed that you have some practise building DL models, at least you have been doing this for a month upwards. This way, and at this point you would know the process of building models with CNNs, RNNs, ML Algorithms and extended libraries and also saving models in your preferred format. Just in case this is not the case for you or perhaps you just need some brush up summary, I would dive briefly explaning how to build deep learning models.&lt;/p&gt;

&lt;p&gt;In this article, I refer to DL models built with Tensorflow, on top of Keras. I use this for two main reasons. One is that, I have the best practise uilding models with Tensorflow and Two, I believe that keras has very easy steps to build models and there exist many methods for deployment and scalable predictions.&lt;/p&gt;

&lt;p&gt;Keras is a high-end library for building ML and DL models. It is built on top of Tensorflow and utilizes itâ€™s many tools, functions and libraries for data loading,visualization,pre-processing and the wholw process involved in model building.&lt;/p&gt;

&lt;p&gt;Most ML and DL task can be carried out in Keras like Computer Vision, Natural Language Processing, Style Transfer, Transfer Learning, Gans, pretty much any DL task can be carried out using keras. If you want some folllow up tutorials for Dl in keras, I suggest you visit Tensorflow oficial website &lt;a href=&quot;https://www.tensorflow.org/tutorials/quickstart/beginner&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3&gt;Making Predictions: Methods to Utilize a Model for Prediction&lt;/h3&gt;
&lt;p&gt;Making predictions with models is a successding step of model deployment. In a way, it can be used interchangably. â€œI want to deploy my modelâ€, for what?,â€to make predictionsâ€.&lt;/p&gt;

&lt;p&gt;This section holds the main content of this article, where I would give you the methods, merits/demerits and all the necessary steps you would need to take to apply any of the &lt;b&gt;scalale predictions and deployments routes&lt;/b&gt; below. Before we get to that, I want to take a moment to answer some questions beginners usually ask. If you donâ€™t consider yourself a beginner, go ahead and move to the next section.&lt;/p&gt;

&lt;h5&gt;Questions beginners might ask about Deployment&lt;/h5&gt;

&lt;p&gt;â€œIs model deployment necessary to a beginner in ML and DLâ€ - No, this should not be something that you spend time on trying to implement because this is going to waste a lot of your time that could be better invested in practising with simple projects&lt;/p&gt;

&lt;p&gt;â€œCan I deploy a model that doesnâ€™t solve a very worthy real-world problemâ€- I absolutely believe that you can do this if you have the time and resources for it because it serves as a learning process and if you start building 
models solving simple problems, you would soon start building advanced ones if you persist in it.&lt;/p&gt;

&lt;p&gt;â€œWhat is the main point of model deploymentâ€- The main point is to provide the function of your deep learning model to a wide audience of people and that it should be easily accessile from various devices.&lt;/p&gt;

&lt;p&gt;â€œAs a beginner, what would be the point of deploying a model aside from feeling good with myselfâ€- Models are meant to solve problems, so yhh if you can build a model channeled to solve or simplify a particular problem, you can distribute this and have people use it as a paid or free service. This is very logical, and what most business do just in advanced applications.&lt;/p&gt;

&lt;p&gt;These were questions that troubled me when I got to know what deployment is all about. I hope that they could be of some help to you. 
Below I am going to take you through the process of model deployment and how you can go about it. Using this guide, you should get through in a day what would naturally have taken upwards of a week.&lt;/p&gt;

&lt;h3&gt;Methods to Deploy a model for Scalale Prediction&lt;/h3&gt;

&lt;ol&gt;
	&lt;li&gt;TFX(Tensorflow Extended): A TFX pipeline is a sequence of components that implement an ML pipeline which is specifically designed for scalable, high-performance machine learning tasks. That includes modeling, training, serving inference, and managing deployments to online, native mobile, and JavaScript targets. To build this pipeline, a few libraries are tasked to take care of different steps in building the scalable ML Pipeline.They are: &lt;ul style=&quot;list-style-type:circle;&quot;&gt;
		&lt;li&gt;TensorFlow Data Validation for validating, analysing and monitoring ML data at scale. It helps to maintain the health of the ML pipeline&lt;/li&gt;
		&lt;li&gt;TensorFlow Transform for preprocessing data into suitable formats. It involves tokenizing and numerical operations such as normalization.This is what i was referring to at the beginning of the post that every model must be prepared for predictions in a different format&lt;/li&gt;
		&lt;li&gt;TensorFlow Model Analysis for computing visualization and evaluation metrics for models. Before deploment it is necessary to evaluate the quality of the model to ensure it meets desired threshold.&lt;/li&gt;
		&lt;li&gt;TensorFlow Serving to support model versioning and for model updates with a rollback option and multiple models for experimentation via A/B testing, while ensuring that concurrent models achieve high throughput on hardware. What Tensorflow serving does in simple words is that it handles updates to your model, rollback options and pushing of multiple models, in a way, think of it as having similar functions to github.&lt;/li&gt;&lt;/ul&gt;

For beginners, your main area of focus should be &lt;b&gt;tensorflow serving&lt;/b&gt;, all other libraries mentioned is secondary and for more professional projects. In the next few lines, I will give instructions on how to set up a TFX for serving models:

* The main software you need for this is Docker, you can install it from this url &lt;a href=&quot;&quot;&gt;Install Docker&lt;/a&gt;
* You also need to install Tensorflow 2.0 and Keras from their official website
* Finally, you must install tensorflow model server by pulling a docker image. This must be done on a 64-bit machine.
These are the three software you need to set up locally. Refer to this article on how to serve a classification model built tensorflow using Tensorflow serving, this should be a guide you can follow to apply it in your own projects &lt;a href=&quot;https://www.tensorflow.org/tfx/tutorials/serving/rest_simple&quot;&gt;Train and serve a Tensorflow model with Tensorflow Serving&lt;/a&gt;

&lt;h3&gt;Merits &amp;amp; Demerits of Tensorflow Serving as a Deployment method&lt;/h3&gt;
&lt;ul style=&quot;list-style-type: square;&quot;&gt;
	&lt;li&gt;It has a very robust functions that allows for many custom configurations&lt;/li&gt;
	&lt;li&gt;It requires technical know-how to set it up and might look tough to first-timers&lt;/li&gt;
	&lt;li&gt;It comes with tools for visualization, normalization , preprocessing, evaluation, analysis of data. This means that developers can carry out every necessary step with just TFX&lt;/li&gt;
	&lt;li&gt;It makes it very easy to update and rollback models versions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;

&lt;br /&gt;
&lt;li&gt;GCP AI Platform: AI Platform makes it easy for developers, data scientists, and data engineers to streamline their ML workflows. AI Platform helps all users take their projects from ideation to deployment seamlessly. There are four main steps involved:

* &lt;b&gt;Preparation&lt;/b&gt; to store your datasets with BigQuery, then use the built-in Data Labeling Service to label your training data by applying classification, object detection, and entity extraction, etc
* &lt;b&gt;Build&lt;/b&gt; for building your model with GCP Auto ML, a managed Jupyter Notebook service that provides fully configured environments for model development or in our case importing your complete tensorflow model.
* &lt;b&gt;Validation&lt;/b&gt; to validate your model with AI Explanations that helps you understand your model's outputs, verify the model behavior, recognize bias in your models, and get ideas for ways to improve your model and your training data.
* &lt;b&gt;Deployment&lt;/b&gt; to Deploy your models at scale and get predictions from them in the cloud with AI Platform Prediction that manages the infrastructure needed to run your model and makes it available for online and batch prediction requests.

The diagram below describes this process in details: &lt;br /&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;https://cloud.google.com/images/ai-platform/ai_platform.svg&quot; /&gt; &lt;br /&gt;

From this brief introduction to AI Platform, we can see that we need to use only the deployment function of the lot. GCP provides the tools to prepare, train and validate but we can do all of these directly in keras when building a model. To get started with AI Platform for Deployment, visit &lt;a href=&quot;https://cloud.google.com/ai-platform/docs/getting-started-keras&quot;&gt;here&lt;/a&gt;. The main advantage of AI Platform is that it is easier to use because most of the code is abstracted and handled b gcp under-the-hood. 
&lt;b&gt;Note:&lt;/b&gt; GCP has it's pricing for all products, if you are just looking to experiment or start of somewhere you can use the 1 year bonus on gcp platform where you can use most products for free.
&lt;/li&gt;

&lt;br /&gt;
&lt;li&gt;Digital Ocean: The final option listed here is perhaps the easiest one to use. It is a very suitable options for beginners and small-medium sized enterprises. Yhh, you also don't have to read pages of documentation. All the work is done behind the hood, usually you just have to click a few options, write some code and that's it. Digital Ocean is by far the most popular cloud hosting service developers use, there are other options you can go for like Katerama, V etc.Digital Ocean offers a free 2 months trial period to test their services, this is a good way to learn how deployment and predictions work before been charged for it. To go about this process, you can refer to this article &lt;a href=&quot;https://towardsdatascience.com/building-a-web-application-to-deploy-machine-learning-models-e224269c1331&quot;&gt;Train and Deploy a Ml model built with Keras&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thatâ€™s it, with these guidelines, the whole process of deplotyment should be less intimidating to you. As a deep learner, this is a very important part of the process, and it is helpful to get accustomed to the process earler on.&lt;/p&gt;

&lt;p&gt;Thanks for learning about deployment with me today, it is understandable if you have a few questions on your mind after taking in this huge pile of guidelines and directions, donâ€™t hesitate to leave me a comment and Iâ€™d be sure to get back to you. Chel_&lt;/p&gt;</content><author><name>Chel</name></author><summary type="html">Buiding Models is exciting, putting them to use can be very brain-tasking. For people who have mastered model deployment, usually this involves just a few tweaks to the code to make it work for a new problem and situation. For beginners like you and me, the task of scaling models for prediction for use in different environment requires a lot of information gathering, software set-up and a little bit of technical know-how. In this article, my aim is to simplify this process for you and show you the methods and steps you can immediately put to use in your projects.</summary></entry><entry><title type="html">5 Books to Start your Deep Learning Journey in 2020</title><link href="http://localhost:4000/books-deep-learning-2020/" rel="alternate" type="text/html" title="5 Books to Start your Deep Learning Journey in 2020" /><published>2020-09-08T00:00:00+01:00</published><updated>2020-09-08T00:00:00+01:00</updated><id>http://localhost:4000/books%20deep%20learning%202020</id><content type="html" xml:base="http://localhost:4000/books-deep-learning-2020/">&lt;p&gt;No doubt, AI is one of the most crucial future technologies which is being harnessed today and transforming our daily lives. A very important branch of Artificial Intelligence â€œDeep Learningâ€ has arguably the biggest impact in AI progress.  If youâ€™re a beginner looking for the right free resources to learn deep learning and discover applications in fields like Computer Vision, NLP, this plog post is you. This article will introduce the best books to learn deep learning in 2020 and how to practice deep learning with them.&lt;/p&gt;

&lt;h4&gt;Overview&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Gentle introduction to Deep Learning&lt;/li&gt;
  &lt;li&gt;Fields of Deep Learning&lt;/li&gt;
  &lt;li&gt;Top 5 Books on Deep Learning (2020)&lt;/li&gt;
  &lt;li&gt;What next after Reading&lt;/li&gt;
  &lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now that you have an idea of the steps taking us through this article, Letâ€™s begin with it.&lt;/p&gt;

&lt;h4&gt;Gentle Introduction to Deep learning&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_learning&quot;&gt;Deep learning&lt;/a&gt; (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.&lt;/p&gt;

&lt;p&gt;Ok, so thatâ€™s more like a very official definition of deep learning. An easier one you should understand better; Deep learning refers to the various process and methods involving the use of Artificial Neural Networks to build models, functions that learn from real-world data and produce outputs closely related to what they were trained on. Now this is probably a vague definition for it, in the sense that some deep learning models use unsupervised method of mapping functions. I could go around and cite a perfect definition for it, but honestly that is usually rarely important for you and me. If you understand the above definitions, you are very well 60% ahead on what deep learning is all about because anything further than that, is already talking about a specific branch of deep learning like computer vision, natural language processing etc.&lt;/p&gt;

&lt;h4&gt;Fields of Deep Learning&lt;/h4&gt;
&lt;p&gt;We come now to a place that we can spend lots of time defining concepts and their differences to each other. The fields or branches of deep learning are many and new theories keeps popping every now and then. This is so because, scientists and researchers are constantly trying to improve upon state-of-the-art models and also achieve new milestones. Several years ago, I should think the world was focusing more on how to improve traditional Machine Learning models and eliminate the repetitive â€œ&lt;b&gt;Rules and Data In - Answers Out&lt;/b&gt;â€ for a scalable solution like â€œ&lt;b&gt;Data and Answers In - Models Out&lt;/b&gt;â€. Now we have Models that are scalable and more confident in unseen situations. I could still remember when I took the course &lt;a href=&quot;https://www.coursera.org/learn/machine-learning/&quot;&gt;Machine Learning&lt;/a&gt; by Andrew Ng, it was everything I was looking for, I was very happy with it, a few months later I took the course &lt;a href=&quot;https://www.coursera.org/learn/introduction-tensorflow/&quot;&gt;Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning&lt;/a&gt; and I could immediately see the huge difference. The difference being using traditional ML methods against Convolutional Neural Networks. Ever since then Iâ€™ve been building models with Tensorflow, Keras and high-end libraries for ML &amp;amp; DL in Skicit-learn and python. Why? Because it is so much easier being that models are scalable, eliminates code repetition and is even easier to train.&lt;/p&gt;

&lt;p&gt;Not to go out of scope, the field of Deep Learning is diverse and keeps growing, here I would list out a few of the more common ones that I have at one time or other come across. I am not going to be explaining them in detail, this post is more focused on Deep learning and how to get started as a deep learner and the books on deep learning to read that would guide your progress:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Computer Vision: Computer Vision, often abbreviated as CV, is defined as a field of study that seeks to develop techniques to help computers â€œseeâ€ and understand the content of digital images such as photographs and videos.The problem of computer vision appears simple because it is trivially solved by people, even very young children. Nevertheless, it largely remains an unsolved problem based both on the limited understanding of biological vision and because of the complexity of vision perception in a dynamic and nearly infinitely varying physical world.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Natural Language Processing: It is a discipline that focuses on the interaction between data science and human language, and is scaling to lots of industries. Today NLP is booming thanks to the huge improvements in the access to data and the increase in computational power, which are allowing practitioners to achieve meaningful results in areas like healthcare, media, finance and human resources, among others.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Generative Adversarial Networks: Generative modeling is an unsupervised learning task in deep learning that involves automatically discovering and learning the regularities or patterns in input data in such a way that the model can be used to generate or output new examples that plausibly could have been drawn from the original dataset.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These are just a few of the many different fields-of-study in deep learning, at least the most common ones. The main context of this post is to introduce the best books about deep learning that can guide your progression in this field, it would simplify a lot of concepts and more important, the information would be easy to digest since it is coming from just one source. It is not an unknown truth that it can be really overwhelming when trying to learn a new skill online usually from different sources. I think that choosing and learning in sequence from an equipped book would be a very good way to start.&lt;/p&gt;

&lt;h4&gt;Top 5 Deep Learning Books in 2020 to learn Deep Learning&lt;/h4&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h5 style=&quot;font-weight: 600;&quot;&gt;Deep Learning With Python&lt;/h5&gt;
&lt;p&gt;&lt;a href=&quot;https://amzn.to/3hbaaJV&quot;&gt;&lt;img src=&quot;https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2016/05/DeepLearningWithPython-400.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h6&gt;Summary:&lt;/h6&gt;

&lt;p&gt;Deep learning is the most interesting and powerful machine learning technique right now.
Top deep learning libraries are available on the Python ecosystem like Theano and TensorFlow 2. Tap into their power in a few lines of code using Keras, the best-of-breed applied deep learning library.&lt;/p&gt;

&lt;p&gt;In this mega Ebook is written in the friendly Machine Learning Mastery style that youâ€™re used to, learn exactly how to get started and apply deep learning to your own machine learning projects. After purchasing you will get:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Read on all devices: English PDF format EBook, no DRM.&lt;/li&gt;
  &lt;li&gt;Tons of tutorials: 20 step-by-step lessons, 266 pages.&lt;/li&gt;
  &lt;li&gt;Projects: 8 end-to-end projects&lt;/li&gt;
  &lt;li&gt;Working code: 71 Python (.py) code files included.&lt;/li&gt;
  &lt;li&gt;Finally, Bring Deep Learning To Your Projects&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Link:&lt;/strong&gt; &lt;a href=&quot;https://machinelearningmastery.com/deep-learning-with-python/&quot;&gt;MachineLearningmstery&lt;/a&gt;&lt;/p&gt;
&lt;div&gt;&lt;/div&gt;
&lt;hr /&gt;

&lt;h5 style=&quot;font-weight: 600;&quot;&gt;Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow 2&lt;/h5&gt;
&lt;p&gt;&lt;a href=&quot;https://amzn.to/35dNiXP&quot;&gt;&lt;img src=&quot;https://images-na.ssl-images-amazon.com/images/I/4184nt3zoGL._SX404_BO1,204,203,200_.jpg&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h6&gt;Summary:&lt;/h6&gt;
&lt;p&gt;Applied machine learning with a solid foundation in theory. Revised and expanded for TensorFlow 2, GANs, and reinforcement learning.&lt;/p&gt;

&lt;p&gt;Python Machine Learning, Third Edition is a comprehensive guide to machine learning and deep learning with Python. It acts as both a step-by-step tutorial, and a reference youâ€™ll keep coming back to as you build your machine learning systems.&lt;/p&gt;

&lt;p&gt;Packed with clear explanations, visualizations, and working examples, the book covers all the essential machine learning techniques in depth. While some books teach you only to follow instructions, with this machine learning book, Raschka and Mirjalili teach the principles behind machine learning, allowing you to build models and applications for yourself.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Link:&lt;/strong&gt; &lt;a href=&quot;https://amzn.to/35dNiXP&quot;&gt;Amazon&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h5 style=&quot;font-weight: 600;&quot;&gt;Deep Learning (MIT Press Essential Knowledge series)&lt;/h5&gt;
&lt;p&gt;&lt;a href=&quot;https://amzn.to/32a5hwr&quot;&gt;&lt;img src=&quot;https://images-na.ssl-images-amazon.com/images/I/41qRN93MrqL._SX355_BO1,204,203,200_.jpg&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h6&gt;Summary:&lt;/h6&gt;
&lt;p&gt;An accessible introduction to the artificial intelligence technology that enables computer vision, speech recognition, machine translation, and driverless cars.&lt;/p&gt;

&lt;p&gt;Kelleher explains that deep learning enables data-driven decisions by identifying and extracting patterns from large datasets; its ability to learn from complex data makes deep learning ideally suited to take advantage of the rapid growth in big data and computational power. Kelleher also explains some of the basic concepts in deep learning, presents a history of advances in the field, and discusses the current state of the art. He describes the most important deep learning architectures, including autoencoders, recurrent neural networks, and long short-term networks, as well as such recent developments as Generative Adversarial Networks and capsule networks. He also provides a comprehensive (and comprehensible) introduction to the two fundamental algorithms in deep learning: gradient descent and backpropagation. Finally, Kelleher considers the future of deep learningâ€•major trends, possible developments, and significant challenges.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Link:&lt;/strong&gt; &lt;a href=&quot;https://amzn.to/32a5hwr&quot;&gt;Amazon&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h5 style=&quot;font-weight: 600;&quot;&gt;Deep Learning with TensorFlow 2 and Keras: Regression, ConvNets, GANs, RNNs, NLP, and more with TensorFlow 2 and the Keras API&lt;/h5&gt;
&lt;p&gt;&lt;a href=&quot;https://amzn.to/3bDt6Q3&quot;&gt;&lt;img src=&quot;https://images-na.ssl-images-amazon.com/images/I/516ibqhWJNL._SX404_BO1,204,203,200_.jpg&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h6&gt;Summary:&lt;/h6&gt;
&lt;p&gt;Build machine and deep learning systems with the newly released TensorFlow 2 and Keras for the lab, production, and mobile devices&lt;/p&gt;

&lt;p&gt;Deep Learning with TensorFlow 2 and Keras, Second Edition teaches neural networks and deep learning techniques alongside TensorFlow (TF) and Keras. Youâ€™ll learn how to write deep learning applications in the most powerful, popular, and scalable machine learning stack available.
This book also introduces neural networks with TensorFlow, runs through the main applications (regression, ConvNets (CNNs), GANs, RNNs, NLP), covers two working example apps, and then dives into TF in production, TF mobile, and using TensorFlow with AutoML.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Link:&lt;/strong&gt; &lt;a href=&quot;https://amzn.to/3bDt6Q3&quot;&gt;Amazon&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;p&gt;Thatâ€™s it, the best 5 deep learning as of 2020. This list has been compiled based on reviews, price and I have had the chance to read a few of them myself. I hope these provides some assistance in your goal to learn and apply deep learning to real-world problems. Of course, it would take time, the time is relatively comparable to how much time is spent learning, practicing and building models.&lt;/p&gt;

&lt;h3&gt;What Next? &lt;/h3&gt;

&lt;p&gt;Now youâ€™ve read some books hopefully, in reality you are yet to though. The truth is quite simple , reading is never the same thing as doing or practicing. Sure you are gonna get a lot of guidelines, understanding of deep learning from reading some of the best books around, but if your goal centers around becoming an expert in a field of deep learning, then you need to work with it every other day. I am not 100% qualified to give directions that I can guarantee would be perfect for everyone, because in a way, there is always a unique situation and way of doing things everyone has. The helpful thing for me is that Iâ€™ve passed through every single step myself and still doing. What I can say is this, as a beginner trying to have expertise in deep learning, the easiet way to progress is to specialize in one or two fields at first. How you can go about this, is finding that one area that interests you, it doesnâ€™t have to be a huge interest at first, because interest and passion can grow as you practice deeper with it.&lt;/p&gt;

&lt;p&gt;I took me time to discover what I loved best in the many branches of AI &amp;amp; DL. It can be a daunting process, daunting because you more than anything want to learn Deep Learning, but have no idea how to start. It can be very overwhelming if you try to have a go at â€œAllâ€ your areas of interest, because In truth, deep learning is fascinating - every part of it.&lt;/p&gt;

&lt;p&gt;Hereâ€™s the action I hope you can take to guide your progress:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Start by Reading one of the recommended books, this would get you through the basics, concepts, branches of deep learning, some theories and even some practical approaches to deep learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Try to choose a field you want to start working in, but as much as this is an important step, it can also be helpful to dance around a bit, to find what you love best before making a choice - At the end of the day, regardless of what you choice is, be openminded in all cases. Now I would give an example of why I said so: My Interest in Deep learning is Computer Vision and Image Synthesis e.g Gans, at least thatâ€™s where my major interest lies. I avoided kaggle community because I thought most of the projects there were limited to â€œData-Science and Machine learningâ€. Yh, I was quite ignorant and was strictly into Computer Vision. Short story, I decided to have a shot at kaggle, well, I found out that there were lots of computer vision projects available. And well, as a beginner, I completed some simple Data-science projects, got to learn pandas, scikit-learn and data-visualization libraries. This was great for me because I learnt some new stuffs and actually found out that I loved Visualizing data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Well, the path to mastering Deep learning(deep learning might be too broad to master, letâ€™s just say, your specific field) has many channels, one person can do so many stuffs along the way but get there later than someone who thoretically did less. My grammer isnâ€™t too great here, Iâ€™m also growing in it just like you. What I can do though, is refer you to a great post by Jason Brownlee, in his post he talks about the Best way to Learn &amp;amp; Do Deep Learning - Everything in that post alings with my views. &lt;a href=&quot;https://machinelearningmastery.com/what-is-deep-learning/&quot;&gt;Deep learning Getting Started&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Summary&lt;/h4&gt;
&lt;p&gt;In this article,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;I shared with you some highly rated books on Deep Learning&lt;/li&gt;
  &lt;li&gt;We talked about Deep Leraning on the surface with some simplistic definitions&lt;/li&gt;
  &lt;li&gt;I listed a few popular fields in Deep learning (Computer Vision, NLP, Gans)&lt;/li&gt;
  &lt;li&gt;I highlighted some action steps you can take to start learning deep learning in practice&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thanks for reading through this post, I hope most that it would of some help to you. If you would like to ask about some more steps or anything at all , feel free to leave a comment below.&lt;/p&gt;</content><author><name>Chel</name></author><summary type="html">No doubt, AI is one of the most crucial future technologies which is being harnessed today and transforming our daily lives. A very important branch of Artificial Intelligence â€œDeep Learningâ€ has arguably the biggest impact in AI progress. If youâ€™re a beginner looking for the right free resources to learn deep learning and discover applications in fields like Computer Vision, NLP, this plog post is you. This article will introduce the best books to learn deep learning in 2020 and how to practice deep learning with them.</summary></entry><entry><title type="html">Virtual Reality 10 Years from now _ What would VR look like in 2030?</title><link href="http://localhost:4000/virtual-reality-and-the-future-of-humans/" rel="alternate" type="text/html" title="Virtual Reality 10 Years from now _ What would VR look like in 2030?" /><published>2020-08-09T00:00:00+01:00</published><updated>2020-08-09T00:00:00+01:00</updated><id>http://localhost:4000/virtual%20reality%20and%20the%20future%20of%20humans</id><content type="html" xml:base="http://localhost:4000/virtual-reality-and-the-future-of-humans/">&lt;p&gt;Virtual Reality is a fascinating field among the many branches of Artificial Intelligence. It is a question if it doesnâ€™t stand on the same level as AI and can bring us tremendously big advantages as we see AI doing now in 2020. At the moment, Virtual Reality has already been adopted in a few industries like the gaming, construction, production, housing among others. For VR to become â€œthe next big thingâ€ in tech it has to be able to generate a huge value for industries. This is only logical as, increasing values means big investments made by invetors which in turn brings advancement to the field by research, building ease-of-use tools and opportunities. The way I see it is this, I have so much hope on VR becoming a massive technological field for so many reasons.&lt;/p&gt;

&lt;p&gt;For Virtual Reality to be embraced by majority if not all industry, it has to impact them by bringing progression and developments in lesser amount of time. I think that currently VR can find application in many sectors but not all have adopted it seeing it as a relatively new technology, in short, a lot of skeptisism surrounds VR. But it should not be that way. Looking at it from a strategic viewpoint, you can see that generally, history always repeats itself. In 1998 when the internet was just taking off, many businesses feared to integrate it in their business and transfer their presence online, and now?, Every business that desires more customers simply knows that the solution is E-bussiness with good advertising and customer targeting. Another example can be seen in Artificial Intelligence. In 1997 IBMâ€™s Deep Blue system defeats the world champion of chess, 20 Years Later, the advancement of AI is massive alomost mind-blowing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\assets\images\Blog\Blog-img\ai-taking-off1.webp&quot; class=&quot;img-fluid&quot; alt=&quot;Annotation_File_Showing_Bounding_Box_position_In_Kangaroo_Dataset&quot; width=&quot;100%&quot; height=&quot;60vh&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Artificial Intelligence with itâ€™s many sub-fields is the main talk and development force of every single industry in the world today. I see Virtual Reality having the same prospects in just a few years time and I want to be a participating force in this nurturing period of VR. Below I would give some exciting ideas that might seem even fantasical to some people, but after seeing past happenings and also knowing that the world we live in demands technology that can shift our concepts of what is possible, I have no reason to not believe in them.&lt;/p&gt;

&lt;p&gt;Before you go on to read these future applications of VR, know that I donâ€™t subject my mind to fantasy thinking rather I use my imaginations in a logical way. Another point is this, I am not just writing for that sake with the hope that in time to come, these technologies would be available to all, and I can be a part of the world that uses it to be more productive or just for fun, No, it is my goal to contribute to innovations in VR and these are no exception. For the fact that you are now reading this blog post, you most likely have a mind just like me and youâ€™re generally interested in Virtual Reality.&lt;/p&gt;

&lt;p&gt;A few days ago, I read an article â€œ&lt;a href=&quot;https://www.incomediary.com/business-startup-lessons-elon-musk&quot;&gt;Elon Muskâ€™s 15 Lessons for your Startup Business&lt;/a&gt;â€ and among the lessonâ€™s a phrase that stood out for me was â€œThink out of this Worldâ€. I have been using my mind in that frame ever since. To the skeptical, you can only see what Elon has achieved by thinking out of this world.&lt;/p&gt;

&lt;h3&gt;Ideas for Virtual Reality that would Change our Paradigm of Reality&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Physical Multiplayer: Here, I refer to mulitplayer games where we humans can compete or collaborate virtually pystically inside a game i.e with our full body and assuming control as in reality. This is something that is extremely exciting for me and of course would be for every other gamers out there. In fact, it introduces a new genre of gaming that could be built on top every other game type that exists. As a reference, this idea was demonstrated in the popular Virtual Reality Movie â€œ&lt;a href=&quot;https://www.imdb.com/title/tt1677720/&quot;&gt;Ready Player one&lt;/a&gt;â€ also in â€œ&lt;a href=&quot;https://www.imdb.com/title/tt1535615/&quot;&gt;Best Player&lt;/a&gt;. I believe that this can be achieved with VR, although it has not be introduced in any games in market today, it should be in development or perhaps in the mind of developers. If one thing, I am interested in the medium that would achieve this. Of course we have unity to build muliplayer VR games but for this idea, I think developers would have to spend more time thinking of the server aspect of this.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Thoughts Projection: What if we can visualize our thoughts or imaginations through a Virtual Reality Medium. This idea can be considered as hogwash to a lot of people and in fact it isnâ€™t founded on so much ground by me. I donâ€™t have the lowest degree in Virtual Reality, but since when did that matter. This idea can be likened to when the wright brothers imagined &lt;b&gt;A bird that would carry humans for long-distances&lt;/b&gt;, it seemed stupendous, even credulous in that age, but not so much today. In fact, what is amazing about it is that after it took them just 3 years to achieve this after it was first conceived as an idea. So what exactly do i mean by â€œThought Projectionâ€. Sometimes it can be hard for we humans to keep a particular train of thought, even worse we can fail to be creative with our imagination. Using the technology called VR, we can have a medium or device that can construct or project meaning visualizations/insights/dashboards/graphics of what it hears. That is, we dictate our thoughts out and they can be transformed to virtual images. The realization of this would be a tremendous achievement for the world!. It would mean more successful innovations as people are using their many ideas in a constructive and creative way.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Virtual Reality is a Technology that brings us closest to a fantasical future. It holds can can bring a lot of developments,fun,innovations to the world. The ideas listed above is something that honestly I see becoming reality in just a few years time. There are other fields of technology that holds big things in time to come like Quantum Computing, Artificial General Intelligence, Computer Vision, Internet of things etc. The combination of these technology would really give us a changed world, by changed world, what I mean is: A world without pollution, untimely death, corruption, terrorism. I argue that no human should lose their lives when it isnâ€™t at the prime of age beacuse life is wonderful and beautiful and all has the right to enjoy it. In short I believe in a world just like in the movie from 2004 â€œWall-Eâ€.&lt;/p&gt;

&lt;p&gt;To developers: The world of Virtual Reality is immence, it requires great effort and harmonious coorperation for we to see big advancement in it. I am open to teaming up with individuals interested and excited about VR. If you are one, donâ€™t hesitate to reach out to me. You can do that by leaving a comment below or visiting &lt;a href=&quot;https://channelai.netlify.app/about&quot;&gt;Aboutâ€™s&lt;/a&gt; page where my many contacts info are linked.&lt;/p&gt;</content><author><name>Chel</name></author><summary type="html">Virtual Reality is a fascinating field among the many branches of Artificial Intelligence. It is a question if it doesnâ€™t stand on the same level as AI and can bring us tremendously big advantages as we see AI doing now in 2020. At the moment, Virtual Reality has already been adopted in a few industries like the gaming, construction, production, housing among others. For VR to become â€œthe next big thingâ€ in tech it has to be able to generate a huge value for industries. This is only logical as, increasing values means big investments made by invetors which in turn brings advancement to the field by research, building ease-of-use tools and opportunities. The way I see it is this, I have so much hope on VR becoming a massive technological field for so many reasons.</summary></entry><entry><title type="html">How to Build an Object Detection Model using Convolutional Neural Networks and Transfer Learning in Keras with Mask RCNN Library.</title><link href="http://localhost:4000/object-detection-model-in-keras-with-mask-R-CNN/" rel="alternate" type="text/html" title="How to Build an Object Detection Model using Convolutional Neural Networks and Transfer Learning in Keras with Mask RCNN Library." /><published>2020-07-24T00:00:00+01:00</published><updated>2020-07-24T00:00:00+01:00</updated><id>http://localhost:4000/object%20detection%20model%20in%20keras%20with%20mask%20R-CNN</id><content type="html" xml:base="http://localhost:4000/object-detection-model-in-keras-with-mask-R-CNN/">&lt;p&gt;In this tutorial you would see how to apply state-of-the-art R-CNN architecture model built on the MS Coco dataset for Object Detection on a new Dataset. We use Transfer Learning for efficient and faster training epochs. You would learn how to Install the R-CNN Library, install the dataset we want to make predictions on, parse the annotation files for bounding boxes, Train Mask R-CNN Model on dataset using Transfer Learning.&lt;/p&gt;

&lt;h3&gt;Overview&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Gentle introduction to Object Detection&lt;/li&gt;
  &lt;li&gt;Object Detection task in Computer Vision using Mask R-CNN&lt;/li&gt;
  &lt;li&gt;Preparing R-CNN library Model&lt;/li&gt;
  &lt;li&gt;Installing the Dataset (Kangaroo)&lt;/li&gt;
  &lt;li&gt;Training Mask R-CNN Model on Dataset using Transfer Learning&lt;/li&gt;
  &lt;li&gt;Detecting Kangaroo in new Images.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now that you have an idea of the stepss taking us through this tutorial, Letâ€™s begin with it.&lt;/p&gt;

&lt;h3&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Object Detection is a Computer Vision task that involves identifying the presence, location, and class of one or more objects in an image. It is a challenging problem that involves building upon methods for Object recognition (e.g. where are they), object localization (e.g. what are their extent), and object classification (e.g. what are they).&lt;/p&gt;

&lt;p&gt;In this tutorial, we are going to use the Region based Convolutional Neural network (R-CNN) to perform object Detection and idetify kangaroos in images. This pre-built model can be used on other datasets to identify other classes of images. For now, we look at a single class (i.e kangaroo) of image.&lt;/p&gt;

&lt;p&gt;We want our model to predict the objects in unseen images and draw a bounding box around them. As we implement this simple task using a simgle class of image, we can get a better understanding of this task and later move on to larger classes of images like in the Coco Dataset.&lt;/p&gt;

&lt;p&gt;Here, instead of developing an implementation of the R-CNN or Mask R-CNN model from scratch, we can use a reliable third-party implementation built on top of the Keras deep learning framework. The best-of-breed third-party implementations of Mask R-CNN is the Mask R-CNN Project developed by Matterport. The project is open source released under a permissive license (e.g. MIT license) and the code has been widely used on a variety of projects and Kaggle competitions. Letâ€™s Begin.&lt;/p&gt;

&lt;p&gt;For this tutorial, some libraries used are not avvailable in tensorflow 2x, please use 1x for this tutorial. Ro change the version you arerunning in colab, use the code below. If you are following this guide on your machine you can create a virtual and install tensorflow 1x.&lt;/p&gt;

&lt;h3&gt;Install Tensorflow 1x&lt;/h3&gt;
&lt;pre class=&quot;css&quot;&gt;&lt;code&gt;
#%tensorflow_version 1.x
#import tensorflow as tf
print(tf.__version__)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Installing the R-CNN library&lt;/h3&gt;
&lt;p&gt;This involves:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cloning github repo&lt;/li&gt;
  &lt;li&gt;Running the installation script&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To git clone in Colab like I am doing, use the code below. If you are doing this locally on your machine, write the code without the exclamation mark â€œgit clone https://github.com/matterport/Mask_RCNN.gitâ€&lt;/p&gt;

&lt;pre class=&quot;css&quot;&gt;&lt;code&gt;
	!git clone https://github.com/matterport/Mask_RCNN.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After running the above,, a new directory â€œMask_RCNNâ€ should appear in the files section on colab. And like-wise, a local-directory should be created on your machine. It comes with the following folders:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;assets&lt;/li&gt;
  &lt;li&gt;images&lt;/li&gt;
  &lt;li&gt;samples&lt;/li&gt;
  &lt;li&gt;mrcnn etc..
Next, we can install the library by moving into Mask_RCNN directory and running the code below.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you are using Google Colab for this guide,, to change directory use the last two lines in the code below,, otherwise use the first two after uncommenting them.&lt;/p&gt;

&lt;pre class=&quot;css&quot;&gt;&lt;code&gt;
#cd /content/Mask_RCNN/
#python setup.py install
%cd /content/Mask_RCNN
!python setup.py install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After running this, the success messages confirms that you installed the library successfully and that you have the latest version, which at the time of writing is version 2.1.&lt;/p&gt;

&lt;p&gt;To be sure, we can confirm the library installed properly using the code below.&lt;/p&gt;

&lt;pre class=&quot;css&quot;&gt;&lt;code&gt;
#Use first line if your are implementing on a local machine, other wise use the second line
#pip show mask-rcnn
!pip show mask-rcnn
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Preparing dataset for Object detection&lt;/h3&gt;
&lt;p&gt;Next, we would start to work with the dataset. We are using the Kangaroo dataset by Huynh Ngoc Anh. It consists of 183 photographs that contain kangaroos, and XML annotation files that provide bounding boxes for the kangaroos in each photograph. We would use mask-rcnn model to perform object detection on the dataset. The R-CNN library is built to perform both object detection tasks and masking, but the kangoroo datatset does not provide mask so we would be focusing on the object detection task here.&lt;/p&gt;

&lt;p&gt;In the next few sections, we look at how to prepare the dataset by downloading, parsing annotations and developing a KangarooDataset object that can be used by Mask_RCNN library.&lt;/p&gt;

&lt;h3&gt;Installing the Dataset&lt;/h3&gt;
&lt;p&gt;Follow the code below to download the kangaroo dataset by cloning it directly from the github repo. A kangaroo directory is created with two sub-directories images and annotes which contain JPEG photos of kangaroos and XML files describing the locations of their bounding boxes respectively.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;css&quot;&gt;
#git clone https://github.com/experiencor/kangaroo.git
!git clone https://github.com/experiencor/kangaroo.git
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Parsing Annotation File&lt;/h3&gt;
&lt;p&gt;Next, we can parse the annots subdirectory containing the size and locations of each object in pictures from the dataset. We can use Python ElementTree API to get fast results. ElementTree API can be used to load and parse an XML file and we can use the find() and findall() functions to perform the XPath queries on a loaded document.&lt;/p&gt;

&lt;p&gt;Letâ€™s understand how this would work by looking at a file from annots. Open up the first file â€œ00001.xmlâ€. On Colab, you can download the file and open it with Sublime text or any other text-editor. It looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\assets\images\Blog\Blog-img\Kangaroo_Dataset_XML_file_1.webp.png&quot; class=&quot;img-fluid&quot; alt=&quot;Annotation_File_Showing_Bounding_Box_position_In_Kangaroo_Dataset&quot; width=&quot;100%&quot; height=&quot;60vh&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can see that it has a size, two objects and two bndbox elements. This is what would define ;&lt;/p&gt;

&lt;p&gt;The size of the image
The number of objects in a photo
The bounding box location of each object.
First, the annotation file must be loaded and parsed as an ElementTree object. Use the code below to do that.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;css&quot;&gt;
from xml.etree import ElementTree
import os
# load and parse the file
path = '/content/Mask_RCNN/kangaroo/annots'
for filename in os.listdir(path):
    if not filename.endswith('.xml'): continue
fullname = os.path.join(path, filename)
tree = ElementTree.parse(fullname)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once loaded, we can retrieve the root element of the document from which we can perform our XPath queries.&lt;/p&gt;

&lt;pre class=&quot;css&quot;&gt;&lt;code&gt;
# get the root of the document
root = tree.getroot()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can use the findall() function with a query for â€˜.//bndboxâ€˜ to find all â€˜bndboxâ€˜ elements, then enumerate each to extract the x and y, min and max values that define each bounding box.&lt;/p&gt;

&lt;p&gt;The element text can also be parsed to integer values.&lt;/p&gt;

&lt;p&gt;We can then collect the definition of each bounding box into a list (coors)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
	# extract each bounding box
	for box in root.findall('.//bndbox'):
	xmin = int(box.find('xmin').text)
	ymin = int(box.find('ymin').text)
	xmax = int(box.find('xmax').text)
	ymax = int(box.find('ymax').text)
	coors = [xmin, ymin, xmax, ymax]
	print(coors
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Image Dimensions may also be needed, we can query that directly&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
# extract image dimensions
width = int(root.find('.//size/width').text)
height = int(root.find('.//size/height').text)
print(width,height)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For ease of use, we can define a function that that takes all the process above and implements it in the steps below when it is called.&lt;/p&gt;

&lt;p&gt;Get annotation filename as an argument
Extract the bounding box using the xmin, ymin, xmax, ymax values.
Extract the image dimension details from the width &amp;amp; height values.
Return them for use
The function extract_all() in the code cell below implements this behaviour.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
	def extract_all(filename):
	# load and parse the file
	tree = ElementTree.parse(filename)
	# get the root of the document
	root = tree.getroot()
	# extract each bounding box
	boxes = list()
	for box in root.findall('.//bndbox'):
	xmin = int(box.find('xmin').text)
	ymin = int(box.find('ymin').text)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can test out this function on our annotation files. Letâ€™s pick the first file for this test.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
# extract details form annotation file
boxes, w, h = extract_all('kangaroo/annots/00001.xml')
# summarize extracted details
print(boxes, w, h)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Developing Kangaroo Dataset Object
The mask-rcnn library requires that train, validation, and test datasets be called by a mrcnn.utils.Dataset object. This means that a new class must be defined that extends the mrcnn.utils.Dataset class and defines a function to load the dataset, with any name you like such as load_dataset(), and override two functions, one for loading a mask called load_mask() and one for loading an image reference (path or URL) called image_reference().&lt;/p&gt;

&lt;h1 id=&quot;class-that-defines-and-loads-the-kangaroo-dataset&quot;&gt;class that defines and loads the kangaroo dataset&lt;/h1&gt;
&lt;p&gt;To use a Dataset object, it is instantiated, then your custom load function must be called, then finally the built-in prepare() function is called.&lt;/p&gt;

&lt;p&gt;Letâ€™s do that in the code cell below.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
# prepare the dataset
#train_set = KangarooDataset()
#train_set.load_dataset(...)
#train_set.prepare()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The custom load function, e.g. load_dataset() is responsible for both defining the classes and for defining the images in the dataset.&lt;/p&gt;

&lt;p&gt;Classes are defined by calling the built-in add_class() function and specifying the â€˜sourceâ€˜ (the name of the dataset), the â€˜class_idâ€˜ or integer for the class (e.g. 1 for the first class as 0 is reserved for the background class), and the â€˜class_nameâ€˜ (e.g. â€˜kangarooâ€˜).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
# define one class
#self.add_class(&quot;dataset&quot;, 1, &quot;kangaroo&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Objects are defined by a call to the built-in add_image() function and specifying the â€˜sourceâ€˜ (the name of the dataset), a unique â€˜image_idâ€˜ (e.g. the filename without the file extension like â€˜00001â€˜), and the path for where the image can be loaded (e.g. â€˜kangaroo/images/00001.jpgâ€˜).&lt;/p&gt;

&lt;p&gt;This will define an â€œimage infoâ€ dictionary for the image that can be retrieved later via the index or order in which the image was added to the dataset. You can also specify other arguments that will be added to the image info dictionary, such as an â€˜annotationâ€˜ to define the annotation path.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
# define one image
# add to dataset
#self.add_image('dataset', image_id='00001', path='kangaroo/images/00001.jpg', annotation='kangaroo/annots/00001.xml')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, we can implement a load_dataset() function that takes the path to dataset directory and loads all image in it. Image with number â€˜00090â€˜ has an issue, so it would be excluded when we write code to load all images. We can also devide the dataset when loading into training and validation sets. The full photos are about 160. In a 80/20 ratio, we are left with about 131 photos in the training set and 20 in validation.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
	# load the dataset definitions
	def load_dataset(self, dataset_dir, is_train=True):
	# define one class
	self.add_class(&quot;dataset&quot;, 1, &quot;kangaroo&quot;)
	# define data locations
	images_dir = dataset_dir + '/images/'
	annotations_dir = dataset_dir + '/annots/'
	# find all images
	for filename in listdir(images_dir):
		# extract image id
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, we need to define the load_mask() function for loading the mask for a given â€˜image_idâ€˜. Here, we use load_mask() function to call an image with its id. The id here is the integer value assigned to all images when loading them from the dataset. They are arranged in incresing order. The id of the image corresponds to that of our annotation, therefore we substitute loading masks which we donâ€™t have for xml files describing bounding boxes for an image.&lt;/p&gt;

&lt;p&gt;To retrieve the annotation on each files we must specify the path to the files, then we can use the function extract_all we defined earlier to get the x,y points in an image describing the bounding boxes and dimensions. Note: self.image_info is a built-in function&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
# get details of image
#info = self.image_info[image_id]
# define box file location
#path = info['annotation']
# load XML
#boxes, w, h = self.extract_boxes(path)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now define a mask value for the images. A mask is a two-dimensional array with the same dimensions as the photograph with all zero values where the object isnâ€™t and one values where the object is.&lt;/p&gt;

&lt;p&gt;We can achieve this by creating a NumPy array with all zero values for the known size of the image and one channel for each bounding box.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
import numpy as np
# create one array for all masks, each on a different channel
masks = zeros([h, w, len(boxes)], dtype='uint8')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each bounding box is defined as min and max, x and y coordinates of the box.&lt;/p&gt;

&lt;p&gt;These can be used directly to define row and column ranges in the array that can then be marked as 1.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
# create masks
#for i in range(len(boxes)):
#	box = boxes[i]
#	row_s, row_e = box[1], box[3]
#	col_s, col_e = box[0], box[2]
#	masks[row_s:row_e, col_s:col_e, i] = 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All objects have the same class in this dataset. We can retrieve the class index via the â€˜class_namesâ€˜ dictionary, then add it to a list to be returned alongside the masks.&lt;/p&gt;

&lt;p&gt;#self.class_names.index(â€˜kangarooâ€™)
Putting all this together,, we have a load_dataset() function that looks like below.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
	# load the masks for an image
	def load_mask(self, image_id):
	# get details of image
	info = self.image_info[image_id]
	# define box file location
	path = info['annotation']
	# load XML
	boxes, w, h = self.extract_boxes(path)
	# create one array for all masks, each on a different channel
	masks = zeros([h, w, len(boxes)], dtype='uint8')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we must implement the image_reference() function.&lt;/p&gt;

&lt;p&gt;This function is responsible for returning the path or URL for a given â€˜image_idâ€™.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
	# load an image reference
	def image_reference(self, image_id):
	info = self.image_info[image_id]
	return info['path']
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And thatâ€™s it. We have successfully defined a Dataset object for the mask-rcnn library for our Kangaroo dataset.&lt;/p&gt;

&lt;p&gt;The complete code for loading our dataset, defining the path for object bounding boxes, creating a mask for images and so on is below&lt;/p&gt;

&lt;h1 id=&quot;split-into-train-and-test-set&quot;&gt;split into train and test set&lt;/h1&gt;
&lt;p&gt;from os import listdir
from xml.etree import ElementTree
from numpy import zeros
from numpy import asarray
from mrcnn.utils import Dataset&lt;/p&gt;

&lt;h1 id=&quot;class-that-defines-and-loads-the-kangaroo-dataset-1&quot;&gt;class that defines and loads the kangaroo dataset&lt;/h1&gt;
&lt;p&gt;class KangarooDataset(Dataset):
	# load the dataset definitions&lt;/p&gt;

&lt;p&gt;Now that we have defined the dataset, letâ€™s confirm that the images, masks, and bounding boxes are handled correctly&lt;/p&gt;

&lt;p&gt;Testing KangarooDataset Object
Now we are gonna confirm that this works properly. We are gonna load an image by specifying an image_id and calling it with the load_image function and likewise, the mask of the image by calling load_mask function with the same image_id. Letâ€™s do that in the code cell below.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
# load an image
image_id = 1
image = train_set.load_image(image_id)
print(image.shape)
# load image mask
mask, class_ids = train_set.load_mask(image_id)
print(mask.shape)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, we can plot the photograph using the Matplotlib Library. We can then plot the mask over the image, we specify an alpha value for the mask here so that it remains transparent and we can still see the image underneath.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
from matplotlib import pyplot
#plot the image
pyplot.imshow(image)
# plot mask
pyplot.imshow(mask[:, :, 0], cmap='gray', alpha=0.5)
pyplot.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The complete code for the example is listed below&lt;/p&gt;

&lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;h1 id=&quot;plot-one-photograph-and-mask&quot;&gt;plot one photograph and mask&lt;/h1&gt;
&lt;p&gt;from os import listdir
from xml.etree import ElementTree
from numpy import zeros
from numpy import asarray
from mrcnn.utils import Dataset
from matplotlib import pyplot&lt;/p&gt;

&lt;h1 id=&quot;class-that-defines-and-loads-the-kangaroo-dataset-2&quot;&gt;class that defines and loads the kangaroo dataset&lt;/h1&gt;
&lt;p&gt;class KangarooDataset(Dataset):
&amp;lt;/b&amp;gt;&lt;/p&gt;

&lt;p&gt;Running the example first prints the shape of the photograph and then the masks.&lt;/p&gt;

&lt;p&gt;We can confirm that both arrays have the same width and height and only differ in terms of the number of channels. We can also see that the first photograph (e.g. image_id=0) in this case only has one mask.&lt;/p&gt;

&lt;p&gt;A plot of the photograph is also created with the first mask overlaid.&lt;/p&gt;

&lt;p&gt;In this case, we can see that one kangaroo is present in the photo and that the mask correctly bounds the kangaroo.&lt;/p&gt;

&lt;p&gt;We could repeat this for the first nine photos in the dataset, plotting each photo in one figure as a subplot and plotting all masks for each photo.&lt;/p&gt;

&lt;h1 id=&quot;plot-first-few-images&quot;&gt;plot first few images&lt;/h1&gt;
&lt;p&gt;for i in range(9):
	# define subplot
	pyplot.subplot(330 + 1 + i)
	# plot raw pixel data
	image = train_set.load_image(i)
	pyplot.imshow(image)
	# plot all masks
	mask, _ = train_set.load_mask(i)
	for j in range(mask.shape[2]):&lt;/p&gt;

&lt;p&gt;Running this plots 9 images with the masks of each placed correctly. Also images with multiple kangaroo objects have separate mask defined.&lt;/p&gt;

&lt;p&gt;Finally, the mask-rcnn library provides utilities for displaying images and masks. We can use some of these built-in functions to confirm that the Dataset is operating correctly.&lt;/p&gt;

&lt;p&gt;For example, the mask-rcnn library provides the mrcnn.visualize.display_instances() function that will show a photograph with bounding boxes, masks, and class labels. This requires that the bounding boxes are extracted from the masks via the extract_bboxes() function.&lt;/p&gt;

&lt;h1 id=&quot;display-image-with-masks-and-bounding-boxes&quot;&gt;display image with masks and bounding boxes&lt;/h1&gt;
&lt;p&gt;from os import listdir
from xml.etree import ElementTree
from numpy import zeros
from numpy import asarray
from mrcnn.utils import Dataset
from mrcnn.visualize import display_instances
from mrcnn.utils import extract_bboxes&lt;/p&gt;

&lt;h1 id=&quot;class-that-defines-and-loads-the-kangaroo-dataset-3&quot;&gt;class that defines and loads the kangaroo dataset&lt;/h1&gt;

&lt;p&gt;Running the example creates a plot showing the photograph with the mask for each object in a separate color.&lt;/p&gt;

&lt;p&gt;The bounding boxes match the masks exactly, by design, and are shown with dotted outlines. Finally, each object is marked with the class label, which in this case is â€˜kangarooâ€˜.&lt;/p&gt;

&lt;p&gt;Training Mask RCNN Model on kangaroo Dataset Using Transfer Learning
We can fit/train a new model using the RCNN architecture from scratch to make predictions on new dataset but that would just be a waste of time. Time can be saved and performance improved by using Transfer Learning.&lt;/p&gt;

&lt;p&gt;The first step to using tranfer learning is to download the weights of our pre-fit in our case RCNN. This can be found in the github project.&lt;/p&gt;

&lt;p&gt;Download here Download Weights (mask_rcnn_coco.h5) 246M&lt;/p&gt;

&lt;p&gt;To download directly to colab without having to upload to colab from your sysatem use the code cell below:&lt;/p&gt;

&lt;p&gt;&lt;b&gt;!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Next we must define a configuration object that extends the mrcnn.config.Config class. It should take in properties for the prediction problem like name and number of class and the algorithm for training like the learning rate.&lt;/p&gt;

&lt;p&gt;The configuration must define the name of the configuration via the â€˜NAMEâ€˜ attribute, e.g. â€˜kangaroo_cfgâ€˜, that will be used to save details and models to file during the run. The number of classes in the prediction problem must also be defined via the â€˜NUM_CLASSESâ€˜ attribute. In this case, we have only one object type of kangaroo,although there is always an additional class for the background.&lt;/p&gt;

&lt;p&gt;Lastly, the number of samples which is the number of training examples or images must be defined, 132 in this case.&lt;/p&gt;

&lt;p&gt;Now after adding the required, our custom KangarooConfig file would look like this:&lt;/p&gt;

&lt;p&gt;[ ]&lt;/p&gt;
&lt;h1 id=&quot;fit-a-mask-rcnn-on-the-kangaroo-dataset&quot;&gt;fit a mask rcnn on the kangaroo dataset&lt;/h1&gt;
&lt;p&gt;from os import listdir
from xml.etree import ElementTree
from numpy import zeros
from numpy import asarray
from mrcnn.utils import Dataset
from mrcnn.config import Config
from mrcnn.model import MaskRCNN&lt;/p&gt;

&lt;h1 id=&quot;class-that-defines-and-loads-the-kangaroo-dataset-4&quot;&gt;class that defines and loads the kangaroo dataset&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;
	
    #define a configuration for the model
    class KangarooConfig(Config):
	# Give the configuration a recognizable name
	NAME = &quot;kangaroo_cfg&quot;
	# Number of classes (background + kangaroo)
	NUM_CLASSES = 1 + 1
	# Number of training steps per epoch
	STEPS_PER_EPOCH = 131
 
# prepare config
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next we would define our model, this is achieved by creating an instance of the mrcnn.model.MaskRCNN class and specifying the model will be used for training via setting the â€˜modeâ€˜ argument to â€˜trainingâ€˜. We would also create an instance of our KangarooConfig file in the config parameter. A directory would then be pointed to where configuration files can be saved and for saving checkpoints of the model at the end of each epochs. the current working directory can be used. Letâ€™s do that now:&lt;/p&gt;

&lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;h1 id=&quot;define-the-model&quot;&gt;define the model&lt;/h1&gt;
&lt;p&gt;model = MaskRCNN(mode=â€™trainingâ€™, model_dir=â€™./â€™, config=config)
&amp;lt;/b&amp;gt;
Before we proceed to loading the model weights, we can define a new variable that would hold the fuction we created earlier that loads the training and test test. We would need this data later when we finally start training. The code below shows how to do this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
# prepare train set
train_set = KangarooDataset()
train_set.load_dataset('kangaroo', is_train=True)
train_set.prepare()
print('Train: %d' % len(train_set.image_ids))
# prepare test/val set
test_set = KangarooDataset()
test_set.load_dataset('kangaroo', is_train=False)
test_set.prepare()
print('Test: %d' % len(test_set.image_ids))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, the pre-define model weights can be loaded, we specify a path to the saved weights â€œmask_rcnn_coco.h5â€, we call the load_weights function on model defined above.&lt;/p&gt;

&lt;p&gt;The model will be used as it is, although the class-specific output layers will be removed so that new output layers can be defined and trained. This can be done by specifying the â€˜excludeâ€˜ argument and listing all of the output layers to exclude or remove from the model after it is loaded. This includes the output layers for the classification label, bounding boxes, and masks.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
# load weights (mscoco)
model.load_weights('mask_rcnn_coco.h5', by_name=True, exclude=[&quot;mrcnn_class_logits&quot;, &quot;mrcnn_bbox_fc&quot;,  &quot;mrcnn_bbox&quot;, &quot;mrcnn_mask&quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, the model can be fit on the training dataset by calling the train() function and passing in both the training dataset and the validation dataset. We can also specify the learning rate as the default learning rate in the configuration (0.001).&lt;/p&gt;

&lt;p&gt;We can also specify what layers to train. In this case, we will only train the heads, that is the output layers of the model.&lt;/p&gt;

&lt;p&gt;[ ]&lt;/p&gt;
&lt;h1 id=&quot;train-weights-output-layers-or-heads&quot;&gt;train weights (output layers or â€˜headsâ€™)&lt;/h1&gt;
&lt;p&gt;model.train(train_set, test_set, learning_rate=config.LEARNING_RATE, epochs=5, layers=â€™headsâ€™)
We could train this for more epochs that fine-tune all of the weights in the model. This could also be achieved by using a smaller learning rate and changing the â€˜layerâ€™ argument from â€˜headsâ€™ to â€˜allâ€™.&lt;/p&gt;

&lt;p&gt;Running this training step can take a bit of time to execute on a modern CPU, if you are running on colab, you can switch to a GPU by clicking the Runtime and selecting change runtime.&lt;/p&gt;

&lt;p&gt;We can see that there are many different train and test loss scores reported for each of the output heads of the network. It can be quite confusing as to which loss to pay attention to.&lt;/p&gt;

&lt;p&gt;In this example where we are interested in object detection instead of object segmentation, I recommend paying attention to the loss for the classification output on the train and validation datasets (e.g. mrcnn_class_loss and val_mrcnn_class_loss), as well as the loss for the bounding box output for the train and validation datasets (mrcnn_bbox_loss and val_mrcnn_bbox_loss).&lt;/p&gt;

&lt;p&gt;A model file is created and saved at the end of each epoch in a subdirectory that starts with â€˜kangaroo_cfgâ€˜ followed by random characters.&lt;/p&gt;

&lt;p&gt;A model must be selected for use; in this case, the loss continues to decrease for the bounding boxes on each epoch, so we will use the final model at the end of the run (â€˜mask_rcnn_kangaroo_cfg_0005.h5â€˜).&lt;/p&gt;

&lt;p&gt;Copy the model file from the config directory into your current working directory. We will use it in the following sections to evaluate the model and make predictions.&lt;/p&gt;

&lt;p&gt;The results suggest that perhaps more training epochs could be useful, perhaps fine-tuning all of the layers in the model.&lt;/p&gt;

&lt;p&gt;Next, letâ€™s look at evaluating the performance of this model.&lt;/p&gt;

&lt;p&gt;Evaluating a Mask R-CNN Model
The performance of a model for an object recognition task is often evaluated using the mean absolute precision, or mAP.&lt;/p&gt;

&lt;p&gt;We are predicting bounding boxes so we can determine whether a bounding box prediction is good or not based on how well the predicted and actual bounding boxes overlap. This can be calculated by dividing the area of the overlap by the total area of both bounding boxes, or the intersection divided by the union, referred to as â€œintersection over union,â€ or IoU. A perfect bounding box prediction will have an IoU of 1.&lt;/p&gt;

&lt;p&gt;It is standard to assume a positive prediction of a bounding box if the IoU is greater than 0.5, e.g. they overlap by 50% or more.&lt;/p&gt;

&lt;p&gt;Precision refers to the percentage of the correctly predicted bounding boxes (IoU &amp;gt; 0.5) out of all bounding boxes predicted. Recall is the percentage of the correctly predicted bounding boxes (IoU &amp;gt; 0.5) out of all objects in the photo.&lt;/p&gt;

&lt;p&gt;As we make more predictions, the recall percentage will increase, but precision will drop or become erratic as we start making false positive predictions. The recall (x) can be plotted against the precision (y) for each number of predictions to create a curve or line. We can maximize the value of each point on this line and calculate the average value of the precision or AP for each value of recall.&lt;/p&gt;

&lt;p&gt;Note: there are variations on how AP is calculated, e.g. the way it is calculated for the widely used PASCAL VOC dataset and the MS COCO dataset differ.&lt;/p&gt;

&lt;p&gt;The average or mean of the average precision (AP) across all of the images in a dataset is called the mean average precision, or mAP.&lt;/p&gt;

&lt;p&gt;The mask-rcnn library provides a mrcnn.utils.compute_ap to calculate the AP and other metrics for a given images. These AP scores can be collected across a dataset and the mean calculated to give an idea at how good the model is at detecting objects in a dataset.&lt;/p&gt;

&lt;p&gt;First, we must define a new Config object to use for making predictions, instead of training. We can extend our previously defined KangarooConfig to reuse the parameters. Instead, we will define a new object with the same values to keep the code compact. The config must change some of the defaults around using the GPU for inference that are different from how they are set for training a model (regardless of whether you are running on the GPU or CPU).&lt;/p&gt;

&lt;p&gt;[ ]&lt;/p&gt;

&lt;h1 id=&quot;define-the-prediction-configuration&quot;&gt;define the prediction configuration&lt;/h1&gt;
&lt;p&gt;class PredictionConfig(Config):
	# define the name of the configuration
	NAME = â€œkangaroo_cfgâ€
	# number of classes (background + kangaroo)
	NUM_CLASSES = 1 + 1
	# simplify GPU config
	GPU_COUNT = 1
	IMAGES_PER_GPU = 1
Next, we can define the model with the config and set the â€˜modeâ€˜ argument to â€˜inferenceâ€˜ instead of â€˜trainingâ€˜.&lt;/p&gt;

&lt;p&gt;[ ]&lt;/p&gt;
&lt;h1 id=&quot;create-config&quot;&gt;create config&lt;/h1&gt;
&lt;p&gt;cfg = PredictionConfig()&lt;/p&gt;
&lt;h1 id=&quot;define-the-model-1&quot;&gt;define the model&lt;/h1&gt;
&lt;p&gt;model = MaskRCNN(mode=â€™inferenceâ€™, model_dir=â€™./â€™, config=cfg)
Next, we can load the weights from our saved model.&lt;/p&gt;

&lt;p&gt;We can do that by specifying the path to the model file. In this case, the model file is â€˜mask_rcnn_kangaroo_cfg_0005.h5â€˜ in the current working directory.&lt;/p&gt;

&lt;p&gt;[ ]&lt;/p&gt;
&lt;h1 id=&quot;load-model-weights&quot;&gt;load model weights&lt;/h1&gt;
&lt;p&gt;model.load_weights(â€˜mask_rcnn_kangaroo_cfg_0005.h5â€™, by_name=True)
Next, we can evaluate the model. This involves enumerating the images in a dataset, making a prediction, and calculating the AP for the prediction before predicting a mean AP across all images.&lt;/p&gt;

&lt;p&gt;First, the image and ground truth mask can be loaded from the dataset for a given image_id. This can be achieved using the load_image_gt() convenience function.&lt;/p&gt;

&lt;p&gt;[ ]&lt;/p&gt;
&lt;h1 id=&quot;load-image-bounding-boxes-and-masks-for-the-image-id&quot;&gt;load image, bounding boxes and masks for the image id&lt;/h1&gt;
&lt;p&gt;image, image_meta, gt_class_id, gt_bbox, gt_mask = load_image_gt(dataset, cfg, image_id, use_mini_mask=False)
Next, the pixel values of the loaded image must be scaled in the same way as was performed on the training data, e.g. centered. This can be achieved using the mold_image() convenience function.&lt;/p&gt;

&lt;p&gt;[ ]&lt;/p&gt;
&lt;h1 id=&quot;convert-pixel-values-eg-center&quot;&gt;convert pixel values (e.g. center)&lt;/h1&gt;
&lt;p&gt;scaled_image = mold_image(image, cfg)
The dimensions of the image then need to be expanded one sample in a dataset and used as input to make a prediction with the model.&lt;/p&gt;

&lt;p&gt;[ ]
sample = expand_dims(scaled_image, 0)&lt;/p&gt;
&lt;h1 id=&quot;make-prediction&quot;&gt;make prediction&lt;/h1&gt;
&lt;p&gt;yhat = model.detect(sample, verbose=0)&lt;/p&gt;
&lt;h1 id=&quot;extract-results-for-first-sample&quot;&gt;extract results for first sample&lt;/h1&gt;
&lt;p&gt;r = yhat[0]
Next, the prediction can be compared to the ground truth and metrics calculated using the compute_ap() function.&lt;/p&gt;

&lt;p&gt;[ ]&lt;/p&gt;
&lt;h1 id=&quot;calculate-statistics-including-ap&quot;&gt;calculate statistics, including AP&lt;/h1&gt;
&lt;p&gt;AP, _, _, _ = compute_ap(gt_bbox, gt_class_id, gt_mask, r[â€œroisâ€], r[â€œclass_idsâ€], r[â€œscoresâ€], r[â€˜masksâ€™])&lt;/p&gt;

&lt;p&gt;The AP Values can be added to a list, then the mean value calculated.&lt;/p&gt;

&lt;p&gt;Tying this together, the evaluate_model() function below implements this and calculates the mAP when given a dataset, model and configuration.&lt;/p&gt;

&lt;p&gt;[ ]&lt;/p&gt;

&lt;h1 id=&quot;calculate-the-map-for-a-model-on-a-given-dataset&quot;&gt;calculate the mAP for a model on a given dataset&lt;/h1&gt;
&lt;p&gt;def evaluate_model(dataset, model, cfg):
	APs = list()
	for image_id in dataset.image_ids:
		# load image, bounding boxes and masks for the image id
		image, image_meta, gt_class_id, gt_bbox, gt_mask = load_image_gt(dataset, cfg, image_id, use_mini_mask=False)
		# convert pixel values (e.g. center)
		scaled_image = mold_image(image, cfg)
		# convert image into one sample&lt;/p&gt;

&lt;p&gt;Running the example will make a prediction for each image in the train and test datasets and calculate the mAP for each.&lt;/p&gt;

&lt;p&gt;A mAP above 90% or 95% is a good score. We can see that the mAP score is good on both datasets, and perhaps slightly better on the test dataset, instead of the train dataset.&lt;/p&gt;

&lt;p&gt;This may be because the dataset is very small, and/or because the model could benefit from further training.&lt;/p&gt;

&lt;p&gt;Detecting Kangaroos in new Images&lt;/p&gt;</content><author><name>Chel</name></author><summary type="html">In this tutorial you would see how to apply state-of-the-art R-CNN architecture model built on the MS Coco dataset for Object Detection on a new Dataset. We use Transfer Learning for efficient and faster training epochs. You would learn how to Install the R-CNN Library, install the dataset we want to make predictions on, parse the annotation files for bounding boxes, Train Mask R-CNN Model on dataset using Transfer Learning.</summary></entry><entry><title type="html">How to treat Overfitting in Convolutional Neural Networks</title><link href="http://localhost:4000/achieving-85-accuracy-on-cifar-10-dataset/" rel="alternate" type="text/html" title="How to treat Overfitting in Convolutional Neural Networks" /><published>2020-07-11T00:00:00+01:00</published><updated>2020-07-11T00:00:00+01:00</updated><id>http://localhost:4000/achieving%2085%20accuracy%20on%20cifar%2010%20dataset</id><content type="html" xml:base="http://localhost:4000/achieving-85-accuracy-on-cifar-10-dataset/">&lt;p&gt;In this tutorial you would learn some methods that can be used to improve a modelâ€™s accuracy using deep learning neural networks. This can be achieved in a few lines of code using Tensorflow by just importing required libraries. If you havenâ€™t already, you can build a Tensorflow Image Classification model following the guide in the last post &lt;a href=&quot;&quot;&gt;How to Build an Image Classification API in Tensorflow&lt;/a&gt;. This is an overview of what we would be touching on in this tutorial:&lt;/p&gt;

&lt;h3&gt;Overview&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Improving Modelâ€™s Accuracy&lt;/li&gt;
  &lt;li&gt;Dropouts: Understanding how it reduces Overfitting&lt;/li&gt;
  &lt;li&gt;Batch Normalization&lt;/li&gt;
  &lt;li&gt;Buiding a Cifar10 Model with 85% Accuracy&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Previous: &lt;a href=&quot;&quot;&gt;How to Build an Image Classification API in Tensorflow&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Next: &lt;a href=&quot;&quot;&gt;Building an Image Classification API with TFX and GCP &lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;Improving Model's Accuracy with Convolutional Neural Networks&lt;/h3&gt;

&lt;p&gt;In the last article we built a simple classifier model in Tensorflow. I achieved 75% accuracy after evaluating the test set which consists of 10000 Images which was not used in training. This gives us a view of how our model would perform in a real world scenario. Of-course, we arenâ€™t going to use this model for any big projects anytime soon, but by practising on it gives us understanding that can be applied to bigger projects.&lt;/p&gt;

&lt;p&gt;Not all modelâ€™s after training can be significantly improved by applying this methods. It could be that the achitecture used at first didnâ€™t train the model in the right way. In this case, you would need to change the layers used before getting any significant improvements. A very good way of knowing if a model can be improved is by plotting learning curves. This can be done in tensorflow in just a few lines of code.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Blog/Blog-img/learning-curves-cifar10-dataset.webp&quot; class=&quot;img-fluid&quot; alt=&quot;Learning Curves Of Cifar10&quot; width=&quot;100%&quot; height=&quot;60vh&quot; /&gt;
As seen in the image above, around epochs 14, the validation set curve (orange) can be seen to have a strong dip. This tells us that if we increase the number of epochs or add regularization techniques like Dropouts and BatchNormalization, we can achieve better accuracy&lt;/p&gt;

&lt;p&gt;In this tutorial, we would look at some established methods of reducing overfitting and slowing-down convergence in our model which in turn gives us better accuracy. Letâ€™s look at them now.&lt;/p&gt;

&lt;h3&gt;Dropouts&lt;/h3&gt;
&lt;p&gt;&lt;b&gt;Dropout&lt;/b&gt; is a technique for addressing overfitting. The main idea here is for our neural network to randomly drop units during training. The reduction inparameters in each step of training has effect of regularization. Dropout has shown improvements in the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.&lt;/p&gt;

&lt;h3&gt;BatchNormalization&lt;/h3&gt;
&lt;p&gt;&lt;b&gt;BatchNormalization&lt;/b&gt; normalizes the activation of the previous layer at each batch, i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1. It addresses the problem of internal covariate shift. It also acts as a regularizer, in some cases eliminating the need for Dropout. Batch Normalization achieves the same accuracy with fewer training steps thus speeding up the training process. This definition is cited from &lt;a href=&quot;https://appliedmachinelearning.blog/2018/03/24/achieving-90-accuracy-in-object-recognition-task-on-cifar-10-dataset-with-keras-convolutional-neural-networks/&quot;&gt;appliedmachinelearning.blog&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now that we understand the technique that would be applied during training to improve accuracy, letâ€™s see how to implement it in code.&lt;/p&gt;

&lt;h3&gt;Buiding a Cifar10 Model with 85% Accuracy&lt;/h3&gt;

&lt;p&gt;Below is the code that introduces batch normalization and dropouts to our modelâ€™s ConvNet. For the full code including loading the dataset and normalizing it, refer to my github repo here &lt;a href=&quot;&quot;&gt;Image Classification with Cifar10 (Full) &lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
from keras.models import Sequential
from keras.optimizers import SGD
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import BatchNormalization
from keras import regularizers
from keras.callbacks import LearningRateScheduler
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D

model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3))),
model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D(2, 2)),
model.add(Dropout(0.2))
model.add(Conv2D(64, (3, 3), activation='relu', padding='same')),
model.add(Conv2D(64, (3, 3), activation='relu', padding='same')),
model.add(MaxPooling2D(2, 2)),
model.add(Dropout(0.2))
model.add(Conv2D(128, (3, 3), activation='relu', padding='same')),
model.add(Conv2D(128, (3, 3), activation='relu', padding='same')),
model.add(MaxPooling2D(2, 2)),
model.add(Dropout(0.2))
model.add(Flatten()),
model.add(Dense(512, activation='relu')),
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))

model.summary()

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

hist = model.fit(x_train, y_train_one_hot, 
          batch_size=64, epochs=25, 
           validation_split=0.2, verbose=1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output of the above code implementation for Image classification task is shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, we trained our model for just 25% accuracy and achieved approx 85% accuracy. This is a significant development from when we only convolution and maxpooling layers to build our model. Training it for more epochs can as well give you more accuracy which you should try out by simply following the guide.&lt;/p&gt;</content><author><name>Chel</name></author><summary type="html">In this tutorial you would learn some methods that can be used to improve a modelâ€™s accuracy using deep learning neural networks. This can be achieved in a few lines of code using Tensorflow by just importing required libraries. If you havenâ€™t already, you can build a Tensorflow Image Classification model following the guide in the last post How to Build an Image Classification API in Tensorflow. This is an overview of what we would be touching on in this tutorial:</summary></entry><entry><title type="html">How to Build an Image Classification Model and Web App with Tensorflow</title><link href="http://localhost:4000/image-classification-api-in-tensorflow/" rel="alternate" type="text/html" title="How to Build an Image Classification Model and Web App with Tensorflow" /><published>2020-06-28T00:00:00+01:00</published><updated>2020-06-28T00:00:00+01:00</updated><id>http://localhost:4000/image%20classification%20api%20in%20tensorflow</id><content type="html" xml:base="http://localhost:4000/image-classification-api-in-tensorflow/">&lt;p&gt;In this tutorial, you would learn how to build an Image Classification Model using Tensorflow on Google Colab. This tutorial is the first among three set. They go from buiding an Image Classifier to Improving accuracy in your model and finally Serving it for production through an API url Endpoint. To start off, here is the overview of all what we would be touching on.&lt;/p&gt;

&lt;h3&gt;Overview&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Image Classification In Computer Vision&lt;/li&gt;
  &lt;li&gt;Applications of Image Classification in World Industry&lt;/li&gt;
  &lt;li&gt;Steps to Build an Image Classification Model&lt;/li&gt;
  &lt;li&gt;Building the Model with Tensorflow (Keras)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Next: &lt;a href=&quot;http://channelai.netlify.app/achieving-85-accuracy-on-cifar-10-dataset/&quot;&gt;Improving your Tensorflow Model with Regularization&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Finally: &lt;a href=&quot;&quot;&gt;Building an Image Classification API with TFX and GCP &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now that you have an idea of the steps taking us through this tutorial, Letâ€™s begin with it.&lt;/p&gt;

&lt;h3&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Starting off in Machine Learning and Deep Learning can be Overwhelming, I say this because Iâ€™m currently in the process of experiencing the start off altogether. There are so many fields, opinions, doubts that can go through your mind on the process of choosing a path to follow. Sometimes, actually most times, Itâ€™s really cool to look at the big guys in the field that exude so much confidence showing that they know what they want. Also, there are lots of resources and articles around promising ways that would become a revelation to newcomers and map out the correct path for them. I just read some of those, and Iâ€™m more affected than when I was using my mind at a space to think for myself. My mood becomes letâ€™s say negative. But I canâ€™t be like that for long. I simply told myself Iâ€™m gonna learn them all by â€œdoingâ€. Itâ€™s a big deal, perhaps the hardest part but with time it becomes a natural thing for you. Thatâ€™s the way of life. Long story short, I decided to start implementing projects that I could add to my github, LinkedIn and blog.&lt;/p&gt;

&lt;p&gt;This tutorial is mainly for people starting off in â€œComputer Visionâ€, those that want to â€œknowâ€ and â€œdoâ€ Computer Vision. What I want you to understand is this: &lt;b&gt;â€œItâ€™s all in the process. Those little steps sums into the big advancementâ€&lt;/b&gt;.. Well, at least I told myself that. OpenCV can wait, Iâ€™m gonna learn all the small steps involved but Iâ€™m gonna learn them very fast. I have the ability to, and I hope with the series of upcoming posts, you too can follow along and master the baby steps in Computer Vision(CV). Letâ€™s get into more concrete writings.&lt;/p&gt;

&lt;h3&gt;Image Classification In Computer Vision&lt;/h3&gt;

&lt;p&gt;The term Image Classification, a deep learning task refers to a task in CV that involves the process of assigning classes to an entire image or picture. It is sometimes referred to as Image recognition, which is a broader term that refers to various tasks that involves classifying the content of images.&lt;/p&gt;

&lt;p&gt;The task of helping computers to see, and make a sense or add meaning to what they see is a very hard task. Mainly because it does not just involve one but a series of steps are involved owning to the fact that real-world data or scenerio is unpredictable. Therefore, this is no longer a common task of pre-programmed rules giving accurate results. For computers to see, at basic level, they have to understand single still images before processing advanced high fps videos of which is our general goal as ML engineers and researchers.&lt;/p&gt;

&lt;p&gt;We come now to the problem of helping computers see. In accord, I decided to start of with the simplest problem and I think that should be Image Classification. In this problem&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;We take a dataset that consists of a set of items in images and their corresponding labels&lt;/li&gt;
  &lt;li&gt;We train a model using the dataset so it can make predictions on new images by correctly assigning them to the right class&lt;/li&gt;
  &lt;li&gt;If it sees an image not among the class it was trained on, it would give it the label of the output with the highest probability&lt;/li&gt;
  &lt;li&gt;This tells us that if we want a model to recognize a thousand different items accurately, we would train it with a dataset outlining those items and their respective label. But ofcourse, this is not an effecient way of tackling the problem. We have millions of items in the world, not to talk of species of animals..xD&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As simple a task as Image Classification is it has some very impressive applications that has been very beneficial to diiferent industries. Letâ€™s look at some Applications:&lt;/p&gt;

&lt;h3&gt;Applications of Image Classification in World Industry&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In Manufacturing: A system built with a classification model can help speed up production process by efficiently reporting bad/damaged items before they are packaged for distribution. By classifying items as bad or good after being trained with a lot of data in the respect of itâ€™s use-case. The main benefit of this is that, it does this considerably faster than human workers and workforce is also reduced resulting in the decrease in â€œcost of productionâ€, which in a way can lead to cheaper products.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In Medical: An image classifiction system trained with a lot of medical X-ray records depicting a patient eith a disease vs one without can be used to accurately diagnose the health condition of new patients. This can significantly reduce the time spent by medical practitioners conducting several tests before giving results, therby giving them more time to focus on other matters.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In Computer Vision: Image classification is a very important application in CV fields. At a basic level, it makes up the structure of an Object detection system which involves â€œlocalizing objects in an image by drawing a bounding box around them while also assigning objects to their correct classâ€. Image classification is used as the basis of many advanced CV applications, a good example is an autonomous vehicle. Image classification and Object detection is the main components making up the system though at advanced level. A self-driving car has to recognize objects it should not hit like pedestrains and road signs, it also has to localize them to undestand their path. This is a clear example of video/moving object detection.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In Agriculture: Models built on Image classification has a tremendous impact in the agricultural sector. They can be applied to numerous problems and can give very accurate results that significantly improves the conditions ans returns of farming. A popular example is an app built in Tensorflow that is used to identify early on, cassave crops with high probability of disease infection. This can help farmers increse yield / decrese loss by being aware of the health of their crops in advance.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are so many other applications of Image classification not mentioned. What I like best about ML models is that they make we humans more productive anywhere which they are applied. And that is why we keep researching ways to make them easier to build, cost effective and being able to be used for more complicated problems.&lt;/p&gt;

&lt;p&gt;Now I just gave some applications, another fun applications is integrating them in web and mobile applications for a number of services partaining to businesses. Today , you would build a classification model and use it as the backend of a web app. Now letâ€™s look at the steps to build this model:&lt;/p&gt;

&lt;p&gt;In this tutorial, Iâ€™m using the Tensorflow ML framework and Keras backend. You dont need to install Tensorflow if you dont have it, we would be writing all the code in Google Colab, you can follow this link to open colab and create a new file â€¦ &lt;a href=&quot;&quot;&gt;Open Google Colab&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;Steps to build an Image Classification Model&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Import Keras and load Cifar10 Dataset&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Understanding our Dataset &amp;amp; Image (shape)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Data Pre-processing&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Image Processing&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Understanding the Stucture of Model&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Building and Training a ConvNet for Image Classification&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;if you are a total beginner and donâ€™t understand some the terms used Like ConvNet, Image Processing and so on, I recommend taking this course by deeplearning.ai on Tensorflow. Or you can contact we directly giving me and I can recommend some resources that should help you get started.&lt;/p&gt;

&lt;p&gt;We want to start by importing Kera which would be what we use to build our model. Cifar10 is a dataset consisting of 10 different items and their corresponding label. We are gonna use this to train a model.&lt;/p&gt;

&lt;p&gt;My aim is to build a web application that uses the complete trained model as a back-end through an API endpoint. This web app would perform object recognition on images when a user uploads a given image and it would then using probability, pass as results the most likely class the object in that image belongs to. As expected, this would only work on 10 different classes of images from real-world examples making the user very limited in the amount of images he can classify( not that he doesnâ€™t know the class of that image). In the next tutorial we can try out thsi same problem but on a larger dataset mainly Cifar100 i.e with 100 class of things. Letâ€™s start writing some code!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use the following lines below to import keras along with cifar10&lt;/li&gt;
  &lt;li&gt;Store the data in the respective arrays to be used for training&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After those simple steps are completed, we can use the print function to see the shape of our training images&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;css&quot;&gt;

from keras.datasets import cifar10
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
print('x_train shape:', x_train.shape)
print('y_train shape:', y_train.shape)
print('no_of_x_train:', len(x_train))

Output: 
x_train shape: (50000, 32, 32, 3)
y_train shape: (50000, 1)
no_of_x_train: 50000S
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By running that piece of code,, we get the following output above. The shape of x_train printed out is (50000, 32, 32, 3). In simple words, the shape of our array tells us that our x_train consists of:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;50000 images (training data)&lt;/li&gt;
	&lt;li&gt;32 pixels in height (images)&lt;/li&gt;
	&lt;li&gt;32 pixels in width&lt;/li&gt;
	&lt;li&gt;3 pixels in depth (RGB values of image)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For our label training data y, we get the shape output as (50000, 1). This tells us we have 50000 label data in our training set with just one value&lt;/p&gt;

&lt;p&gt;That is the shape of our training data but keras stacks them in a 4D format, for easier processing when building our model. Now letâ€™s try to see a full example of an image and label to better understand the logic. The code below helps us to that.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;css&quot;&gt;
print(x_train[0])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output looks like the image below:
&lt;img src=&quot;/assets/images/Blog/Blog-img/Computer_visualization_of_image_in_numbers.webp&quot; class=&quot;img-fluid&quot; alt=&quot;Computer visualization of Image in Numbers&quot; width=&quot;100%&quot; height=&quot;60vh&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Trying to visualize the image this way doesnâ€™t help us a lot, although thatâ€™s how it is interpreted for the computer. The Red, green, and blue values in our 3 byte image are just a bunch of numbers that the computer can use to calculate the intensity of r,g,b at a pixel point. We can use the matplotlib package for a finer, data plot. Letâ€™s import the package in the next lines of code&lt;/p&gt;

&lt;p&gt;&lt;b&gt;&lt;em&gt;Note:&lt;/em&gt;&lt;/b&gt; imshow is a function that maps the numbered pixel values of x_train[0] like we saw previously, into the actual image it represents.We also print out the label, that is, the class this image belongs to of the 10 classes we have. Classes begin from index 0.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;css&quot;&gt;
import matplotlib.pyplot as plt
img = plt.imshow(x_train[1])

print('The label is:', y_train[1])

Output:
The label is: [9]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The label showing as nine means that the second image in our training set (following index: 0 is first_img, 1 is second_img ) has a label/class of 9(truck) i.e the very last of the classes. With this we still do not know the class in words representing this image. The below image shows the conversion of numbers to the word class. Change the value of the printed xâ€™s and labels in your colab environment, and visualize the different classes that gets printed out.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Blog/Blog-img/number_to_word_class_cifar10.webp&quot; class=&quot;img-fluid&quot; alt=&quot;Numbers_to_words_class_representation_ImageClassification_in tensorflow&quot; width=&quot;100%&quot; height=&quot;30vh&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;Data Pre-processing&lt;/h3&gt;

&lt;p&gt;In this step we want to preprocess our data because our labels as class number are not very helpful. We can hot-encode our label so that the model returns 1 for matched class and 0 for the rest unmatched classes(i.e 9,since there would always be one matched class). For a simple binary-logistic predictor, we would not need to perform this pre processing step mainly because our classes are just two. Class 0 and class 1. 
For example, building a model that predicts if a student is given admission(1) or not given(0) by training on past data X(scores on tests) and y(admission decision). In this case, we have only two possible classes and can continue without the need for data procesing.&lt;/p&gt;

&lt;p&gt;We can call our current problem a multi-variant/multi-label logistic problem. Using traditional Machine Learning method of mapping xâ€™s to their labels, we would have to build a classifyer for all our 10 classes and optimize each of them separately,i.e 10 seperate theta parameter. But thankfully, we are using a framework that abstracts much of the work for us.&lt;/p&gt;

&lt;p&gt;This processing step is called one-hot encoding. It involves converting the labels into a set of 10 numbers where each number represents if the image belongs to that class or not. So if an image belongs to the first class, the first number of this set will be a 1 and all other numbers in this set will be a 0. The conversion table would look like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Blog/Blog-img/one-hot-encoding-imageclassification.webp&quot; class=&quot;img-fluid&quot; alt=&quot;one-hot-encoding-imageclassification&quot; width=&quot;100%&quot; height=&quot;30vh&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now here we have been talking about one-hot encoding for a while, letâ€™s write code for it in a few lines:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;css&quot;&gt;
import keras
y_train_one_hot = keras.utils.to_categorical(y_train, 10)
y_test_one_hot = keras.utils.to_categorical(y_test, 10)

print('The one hot label is:', y_train_one_hot[1])

Output: 
The one hot label is: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the above code, the line &lt;b&gt;y_train_one_hot = keras.utils.to_categorical(y_train, 10)&lt;/b&gt; means that we want to take the array conatining the number labels of y_train and convert it to one-hot setting y_train_one_hot. The numeber 10 is a required paarmeter, it specifies the number of classes there are.&lt;/p&gt;

&lt;p&gt;We printed out the label of our second image(index 1/truck/label 9) and got the one-hot setting as above.&lt;/p&gt;

&lt;h3&gt;Processing Images(x)&lt;/h3&gt;
&lt;p&gt;To process our images , what we do is reduce the pixels values between 0 and 1 which would aid in training our neural network. The pixel values of x ranges between 0 - 255, to reduce them we can divide b 255. We first covert the datatype to float32 which is a datatype that can store decimal values. Letâ€™s write code for this.&lt;/p&gt;

&lt;p&gt;We process our images before training mainly for our convolutions to easily find features to use for training therby reducing training period&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;css&quot;&gt;
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

x_train = x_train / 255
x_test = x_test / 255

x_train[0]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When we print our output,, like previously we get a bunch of numbers except that this time around they all fall in the range of 0 and 1. The process of reducing our image pixels in this way is called &lt;b&gt;&lt;em&gt;&lt;a href=&quot;https://channelai.netlify.app/introduction-to-machine-learning.md&quot;&gt;Normalization&lt;/a&gt;&lt;/em&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Blog/Blog-img/normalized_form_of_cifar10_xtrain_after_image_processing.webp&quot; class=&quot;img-fluid&quot; alt=&quot;normalized_form_of_cifar10_xtrain_after_image-processing &quot; width=&quot;100%&quot; height=&quot;30vh&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This processing step makes our neural network to train faster and achieve better accuracy. Note: This processing is also carried out on the test set since when evaluating we want the same range of values as those used when training.&lt;/p&gt;

&lt;h3&gt;Understanding the Stucture of Model&lt;/h3&gt;

&lt;p&gt;We finally arrive at the most productive step, to train our model. This step just involves setting up a structure or architecture we want our model to have. We would add layers to our model in these sequence:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Add Convolution layer with zero-padding: A convolution is the simple application of a filter to an input that results in an activation. Repeated application of the same filter to an input results in a map of actitvation called a feature map, indicating the locations and strength of a detected feature in an input, such as an image.
	A concolution layer can be created by specifying both the number of filters to learn and the fixed size of each filetr,often called the kernel shape. This layer isolates the features in our images so that we can gather more information in less pixels. It actually ends up reducing the size of the pixels. The zero-padding works to produce the same output width and height as that of the input, what this means literally is that; we aren't losing any information by using zero-padding also called same-padding. The kind of padding used would either try to highlight the horizontal or vertical aspect(lines) of the image.&lt;/li&gt;&lt;br /&gt;
	&lt;li&gt;Add MaxPolling layer: Pooling layer provides an approach to downsampling feature maps by summarizing the presence of features in patches of the feature map. Maximum pooling or max pooling, is a pooling operation that calculates the maximum or largest value in each part of each feature map. It works by shrinking the pixel height and weight while still retaining relevant information. It is passed across the whole image, the size of the stride determines the quantity or perhaps quality of features that is passed to the output after MaxPooling is carried out.&lt;/li&gt;&lt;br /&gt;
	&lt;li&gt;Relu activation function: We use the relu activation in all layers except for the last layer which is a softmax. Relu (Rectified Linear Unit) is an activation that outputs the value of input X if x &amp;gt; 0 and outputs 0 if x &amp;lt; 0.&lt;/li&gt;&lt;br /&gt;
	&lt;li&gt;Softmax: Our last layer Softmax simply transforms the output of the previous layer into probability distributions. So we are able to match image classification based on the probability given by the model of each classes. And therefore, our model picks the class with the highest probability as the correct item detected in an image.&lt;/li&gt;&lt;br /&gt;
	&lt;li&gt;Sequential: Keras imports Sequential which is a library that holds all the types of layers we can use to build our model:
	&lt;ul style=&quot;list-style-type: disc;&quot;&gt;
		&lt;li&gt;Dense is the layer that combines all our parameters into a single dense package&lt;/li&gt;
		&lt;li&gt;Flatten is the layer that transforms our two-dimensional feature ma our image shape into one long vector&lt;/li&gt;
		&lt;li&gt;Dropout prevents overfitting in the layers&lt;/li&gt;
	&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Building the Model with Tensorflow (Keras)&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;css&quot;&gt;
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D

model = Sequential()
model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(32, 32, 3))),
model.add(Conv2D(32, (3, 3), activation='relu'))
model.add(MaxPooling2D(2, 2)),
model.add(Dropout(0.25))

model.add(Conv2D(64, (3, 3), activation='relu')),
model.add(Conv2D(64, (3, 3), activation='relu')),
      #model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D(2, 2)),
model.add(Dropout(0.25))

model.add(Flatten()),
model.add(Dense(1024, activation='relu')),
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))



model.summary()

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
hist = model.fit(x_train, y_train_one_hot, 
           batch_size=32, epochs=15, 
           validation_split=0.2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After training , I got an accuracy of 79% on my training set and 75% on my validation set. Itâ€™s quite noticeable that the usage of dropouts after layers helped in minimizing overfitting. Iâ€™m almost underfitted but I can not make that claim as I got quite a good accuracy on my validation set. U should try tweaking different values to fully understand this and see if you can improve the accuracy of your model. Some hyper-parameters that you should consider when tuning includes: epochs, number of ConvNets(add more with higher depths), dropouts.&lt;/p&gt;

&lt;p&gt;Before using this model to make some predictions, letâ€™s plot some learning curves that would help us understand how well our model trained. this step should help us understand overfitting, underfitting, unsteady training and so on. This can help us to decide steps to take that can help improve the model.&lt;/p&gt;

&lt;h4&gt;Plot showing Model Loss during Training&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/Blog/Blog-img/model_loss_plot_learning_curve.webp&quot; class=&quot;img-fluid&quot; alt=&quot;model_loss_plot_learning_curve&quot; width=&quot;100%&quot; height=&quot;30vh&quot; /&gt;
â€¦..&lt;/p&gt;
&lt;p style=&quot;padding-top: 5%&quot;&gt;&lt;/p&gt;
&lt;h4&gt;Plot showing Model Accuracy during Training&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/Blog/Blog-img/model_accuracy_plot_learning_curve.webp&quot; class=&quot;img-fluid&quot; alt=&quot;model_accuracy_plot_learning_curve&quot; width=&quot;100%&quot; height=&quot;30vh&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From the above plots we see that, we had a steady training without overfitting our model.
Now for the last step, we want to evaluate our model using the test set and also make predictions using entirely new images. You can do this by getting images that fall into any one of the classes as we have in our dataset. The code below would enable you to choose multiple files from your storage for prediction.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;css&quot;&gt;
model.evaluate(x_test, y_test_one_hot)[1]
model.save('chel_cifar10_model_OR.h5')

import numpy as np
from skimage import data, color, feature, transform
from skimage.transform import resize
from google.colab import files
from keras.preprocessing import image

uploaded = files.upload()
for fn in uploaded.keys():

path = '/content/' + fn
img = image.load_img(path, target_size=(32, 32))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)

#images = np.vstack([x])
probabilities = model.predict(images, batch_size=10)
print(probabilities[0])

number_to_class = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
index = np.argsort(probabilities[0,:])
print(&quot;Most likely class:&quot;, number_to_class[index[9]], &quot;-- Probability:&quot;, probabilities[0,index[9]])
print(&quot;Second most likely class:&quot;, number_to_class[index[8]], &quot;-- Probability:&quot;, probabilities[0,index[8]])
print(&quot;Third most likely class:&quot;, number_to_class[index[7]], &quot;-- Probability:&quot;, probabilities[0,index[7]])
print(&quot;Fourth most likely class:&quot;, number_to_class[index[6]], &quot;-- Probability:&quot;, probabilities[0,index[6]])
print(&quot;Fifth most likely class:&quot;, number_to_class[index[5]], &quot;-- Probability:&quot;, probabilities[0,index[5]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above code evaluates our model using test set, saves the trained model in a .h5 format and also we are able to upload files for prediction by the files library from keras. Then we converted the output numbers into words_class as itâ€™s easier to understand that way. The last bit of code enables us to do that.&lt;/p&gt;

&lt;p&gt;After training the model and evaluating on the test set (x_test), I got an accuracy of 75%. This means that, our classifier would classify correctly only 75% of new unseen images we feed it.&lt;/p&gt;

&lt;p&gt;Test out different images to see how well the model you built classifies them.&lt;/p&gt;

&lt;p&gt;In the part 2 of this article, letâ€™s look at how to Improve significantly our built modelâ€™s accuracy by Introducing some new concepts that would prevent over-fitting and slow down convergence. &lt;a href=&quot;&quot;&gt;Next Article: Improving your Tensorflow Model with Dropouts and Batch Normalization&lt;/a&gt;&lt;/p&gt;</content><author><name>Chel</name></author><summary type="html">In this tutorial, you would learn how to build an Image Classification Model using Tensorflow on Google Colab. This tutorial is the first among three set. They go from buiding an Image Classifier to Improving accuracy in your model and finally Serving it for production through an API url Endpoint. To start off, here is the overview of all what we would be touching on.</summary></entry></feed>